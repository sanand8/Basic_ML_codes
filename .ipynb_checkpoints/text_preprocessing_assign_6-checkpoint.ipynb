{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a4b76284e3bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_tokenize\u001b[0m                         \u001b[1;31m# Tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m                               \u001b[1;31m# Stop Words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os                                                       # Miscellaneous operating system interfaces\n",
    "import re                                                       # Regular Expressions\n",
    "import random                                                   # Python Random Library\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize                         # Tokenizer\n",
    "from nltk.corpus import stopwords                               # Stop Words\n",
    "from nltk.stem.porter import *                                  # Stemmer - Porter\n",
    "from nltk.stem.snowball import SnowballStemmer                  # Stemmer - Snowball\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "from nltk.util import ngrams\n",
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['comp.graphics','rec.autos','sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"20_newsgroups\"\n",
    "os.path.join(datadir, categories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"20_newsgroups\"\n",
    "\n",
    "paths=[]\n",
    "l=[]\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(datadir, category)\n",
    "    paths.append(path)\n",
    "\n",
    "for path in paths:\n",
    "    for i in range(1):\n",
    "        choice = random.choice(os.listdir(path)) \n",
    "        fullfilename = os.path.join(path, choice)\n",
    "        with open(fullfilename) as f:\n",
    "            l.append(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting list of lists into a flat_list\n",
    "lines = [] \n",
    "for sublist in l:\n",
    "    for item in sublist:\n",
    "        lines.append(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(line, num):\n",
    "    n_grams = ngrams(line, num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk import word_tokenize, pos_tag\n",
    "for i in range(len(lines)):\n",
    "    line=[]\n",
    "    line=re.sub('[^a-zA-Z]',' ',lines[i])\n",
    "    line = line.lower()\n",
    "    line = line.split()#tokenization \n",
    "    print(\"tokenized words\")\n",
    "    print(line)\n",
    "    print(\"\\n\")\n",
    "    line=[word for word in line if word not in set(stopwords.words('english'))]\n",
    "    print(\"Removing Stopwords\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    line = [lemmatizer.lemmatize(word) for word in line]\n",
    "    print(\"Lemmatization\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    line = [ps.stem(word) for word in line]\n",
    "    print(\"Stemming\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    print(\"pos_tags and chunking\")\n",
    "    print(\"After Split:\",line)\n",
    "    tokens_tag = pos_tag(line)\n",
    "    print(\"After pos_tags:\",tokens_tag)\n",
    "    patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "    chunker = RegexpParser(patterns)\n",
    "    print(\"After Regex:\",chunker)\n",
    "    output = chunker.parse(tokens_tag)\n",
    "    print(\"After Chunking\",output)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(line)\n",
    "    print(\"N_grams\")\n",
    "    print(\"2-gram: \", extract_ngrams(line, 2))\n",
    "    print('\\n')\n",
    "    \n",
    "    line= [word for word in line if word in words]#taking dictionary words only to build final bag of word models\n",
    "    line = [word for word in line if len(word)>1]\n",
    "\n",
    "\n",
    "\n",
    "    line=' '.join(line)\n",
    "    if(line):\n",
    "        corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging abbreviations\n",
    "# Abbreviation\tMeaning\n",
    "# CC\tcoordinating conjunction\n",
    "# CD\tcardinal digit\n",
    "# DT\tdeterminer\n",
    "# EX\texistential there\n",
    "# FW\tforeign word\n",
    "# IN\tpreposition/subordinating conjunction\n",
    "# JJ\tadjective (large)\n",
    "# JJR\tadjective, comparative (larger)\n",
    "# JJS\tadjective, superlative (largest)\n",
    "# LS\tlist market\n",
    "# MD\tmodal (could, will)\n",
    "# NN\tnoun, singular (cat, tree)\n",
    "# NNS\tnoun plural (desks)\n",
    "# NNP\tproper noun, singular (sarah)\n",
    "# NNPS\tproper noun, plural (indians or americans)\n",
    "# PDT\tpredeterminer (all, both, half)\n",
    "# POS\tpossessive ending (parent\\ 's)\n",
    "# PRP\tpersonal pronoun (hers, herself, him,himself)\n",
    "# PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "# RB\tadverb (occasionally, swiftly)\n",
    "# RBR\tadverb, comparative (greater)\n",
    "# RBS\tadverb, superlative (biggest)\n",
    "# RP\tparticle (about)\n",
    "# TO\tinfinite marker (to)\n",
    "# UH\tinterjection (goodbye)\n",
    "# VB\tverb (ask)\n",
    "# VBG\tverb gerund (judging)\n",
    "# VBD\tverb past tense (pleaded)\n",
    "# VBN\tverb past participle (reunified)\n",
    "# VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "# VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "# WDT\twh-determiner (that, what)\n",
    "# WP\twh- pronoun (who)\n",
    "# WRB\twh- adverb (how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cats=['comp.graphics','talk.religion.misc','sci.space','alt.atheism']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train',categories = cats)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.filenames.shape\n",
    "\n",
    "newsgroups_train.target.shape\n",
    "\n",
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories = cats)\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "clf = MultinomialNB(alpha = .01)\n",
    "\n",
    "clf.fit(vectors,newsgroups_train.target)\n",
    "\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target,pred,average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "        \n",
    "show_top10(clf, vectorizer, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'),categories = cats)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(pred, newsgroups_test.target, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'),categories=categories)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

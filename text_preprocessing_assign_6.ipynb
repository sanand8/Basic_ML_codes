{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os                                                       # Miscellaneous operating system interfaces\n",
    "import re                                                       # Regular Expressions\n",
    "import random                                                   # Python Random Library\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize                         # Tokenizer\n",
    "from nltk.corpus import stopwords                               # Stop Words\n",
    "from nltk.stem.porter import *                                  # Stemmer - Porter\n",
    "from nltk.stem.snowball import SnowballStemmer                  # Stemmer - Snowball\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "from nltk.util import ngrams\n",
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['comp.graphics','rec.autos','sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20_newsgroups\\\\comp.graphics'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = \"20_newsgroups\"\n",
    "os.path.join(datadir, categories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"20_newsgroups\"\n",
    "\n",
    "paths=[]\n",
    "l=[]\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(datadir, category)\n",
    "    paths.append(path)\n",
    "\n",
    "for path in paths:\n",
    "    for i in range(1):\n",
    "        choice = random.choice(os.listdir(path)) \n",
    "        fullfilename = os.path.join(path, choice)\n",
    "        with open(fullfilename) as f:\n",
    "            l.append(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Newsgroups: comp.graphics',\n",
       "  'Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!uunet!mcsun!sun4nl!sun2.iend.wau.nl!PvdVeen.EL.WAU.NL',\n",
       "  'From: Peter.vanderveen@visser.el.wau.nl  (Peter van der Veen)',\n",
       "  'Subject: Re: Fonts in POV??',\n",
       "  'Message-ID: <19930415162925.Peter.vanderveen@PvdVeen.EL.WAU.NL>',\n",
       "  'Lines: 30',\n",
       "  'Sender: news@sun2.iend.wau.nl (Newsmanager WAU)',\n",
       "  'Organization: Wageningen Agricultural University',\n",
       "  'X-Newsreader: FTPNuz (DOS) v1.0',\n",
       "  'References: <1qg9fc$et9@wampyr.cc.uow.edu.au> ',\n",
       "  'Date: Thu, 15 Apr 1993 20:29:00 GMT',\n",
       "  '',\n",
       "  'In Article <1qg9fc$et9@wampyr.cc.uow.edu.au> \"g9134255@wampyr.cc.uow.edu.au (Coronado Emmanuel Abad)\" says:',\n",
       "  '> ',\n",
       "  '> ',\n",
       "  '> \\tI have seen several ray-traced scenes (from MTV or was it ',\n",
       "  '> RayShade??) with stroked fonts appearing as objects in the image.',\n",
       "  '> The fonts/chars had color, depth and even textures associated with',\n",
       "  '> them.  Now I was wondering, is it possible to do the same in POV??',\n",
       "  '> ',\n",
       "  '> ',\n",
       "  '> Thanks,',\n",
       "  '> ',\n",
       "  '> Noel',\n",
       "  '> ',\n",
       "  'Yes, there are serveral programs which can convert font files (eq the Borland',\n",
       "  'fonts) to objects consisting of spheres, cones etc. ',\n",
       "  \"I've used a program (forgot its name/place, but i can look for it) which\",\n",
       "  'converted these Borland fonts to three different raytracers. Vivid, POV and',\n",
       "  'Polyray (which i like more (more flexibel/faster/use of expressions etc).',\n",
       "  'The program has a lot nice features.',\n",
       "  'So if interested give me a mail.',\n",
       "  '',\n",
       "  ' /*---------*\\\\*/*-------------------------------------------*\\\\',\n",
       "  ' *|  ____/|  *|*    PETER.VANDERVEEN@VISSER.EL.WAU.NL       |*',\n",
       "  ' *|  \\\\ o.O|  *|*    Department of Genetics                  |*',\n",
       "  ' *|   =(_)=  *|*    Agricultural University                 |*',\n",
       "  ' *|     U    *|*    Wageningen, The Netherlands             |*',\n",
       "  ' \\\\*---------*/*\\\\*-------------------------------------------*/'],\n",
       " ['Xref: cantaloupe.srv.cs.cmu.edu rec.autos:102882 rec.autos.tech:54037 alt.autos.antique:2762 rec.autos.antique:1213',\n",
       "  'Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!ames!sun-barr!news2me.EBay.Sun.COM!jethro.Corp.Sun.COM!vavau!barnesm',\n",
       "  'From: barnesm@sun.com (Mark Barnes - SunSoft)',\n",
       "  'Newsgroups: rec.autos,rec.autos.tech,alt.autos.antique,rec.autos.antique',\n",
       "  'Subject: Re: WARNING.....(please read)...',\n",
       "  'Date: 15 Apr 1993 21:02:01 GMT',\n",
       "  'Organization: Sun Microsystems, Inc.',\n",
       "  'Lines: 12',\n",
       "  'Distribution: world',\n",
       "  'Message-ID: <1qkig9$mom@jethro.Corp.Sun.COM>',\n",
       "  'References: <1qke5b$mc4@spool.mu.edu>',\n",
       "  'Reply-To: barnesm@sun.com',\n",
       "  'NNTP-Posting-Host: vavau.corp.sun.com',\n",
       "  '',\n",
       "  '...and in San Francisco recently, some of our finest examples of humanity',\n",
       "  'poured oil over a road so that vehicles going uphill would suddnely become',\n",
       "  'immobile, and then they would walk right up to the vehicles and make their',\n",
       "  'demands known.',\n",
       "  '--------------------------------+---------------------------------------',\n",
       "  'Mark Barnes, System Engineer    |  <insert standard disclaimers here>',\n",
       "  'SunSoft                         |',\n",
       "  'Corporate Technical Escalations |  I speak for myself, an individual,',\n",
       "  'Menlo Park, CA, USA             |  not for the company for which I work.',\n",
       "  'barnesm@vavau.Corp.Sun.COM      |',\n",
       "  '--------------------------------+---------------------------------------',\n",
       "  ''],\n",
       " ['Xref: cantaloupe.srv.cs.cmu.edu sci.environment:30724 misc.consumers:69234 misc.invest:42646 sci.astro:36018 talk.environment:12219 talk.politics.space:2857 sci.space:62448 rec.backcountry:32284 misc.rural:6319 misc.headlines:42250',\n",
       "  'Newsgroups: sci.environment,misc.consumers,misc.invest,sci.astro,talk.environment,talk.politics.space,sci.space,rec.backcountry,misc.rural,misc.headlines,k12.chat.teacher',\n",
       "  'Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!darwin.sura.net!rouge!srl03.cacs.usl.edu!pgf',\n",
       "  'From: pgf@srl03.cacs.usl.edu (Phil G. Fraering)',\n",
       "  'Subject: Re: Space Marketing would be wonderfull.',\n",
       "  'Message-ID: <pgf.737604122@srl03.cacs.usl.edu>',\n",
       "  'Sender: anon@usl.edu (Anonymous NNTP Posting)',\n",
       "  'Organization: Univ. of Southwestern Louisiana',\n",
       "  'References: <FOX.93May15223005@graphics.nyu.edu> <C73rp4.3G2@lysator.liu.se> <16MAY93.11224501@meena.cc.uregina.ca> <pgf.737587842@srl03.cacs.usl.edu> <16MAY93.16432680@meena.cc.uregina.ca>',\n",
       "  'Date: Mon, 17 May 1993 02:02:02 GMT',\n",
       "  'Lines: 50',\n",
       "  '',\n",
       "  'stange@meena.cc.uregina.ca writes:',\n",
       "  '',\n",
       "  '>Phil, your point is well taken.  It is still a sad idea.',\n",
       "  '',\n",
       "  \"I'm worried by the concern about it though, for a number of reasons\",\n",
       "  'that have nothing to do with Space Advertising (which for a number of',\n",
       "  'reasons is probably doomed to fail on financial grounds).',\n",
       "  '',\n",
       "  \"(And I've been reading and (and writing) this thread since way\",\n",
       "  'back when it was only on sci.space).',\n",
       "  '',\n",
       "  \"For starters, I don't think the piece of light-pollution apparatus\",\n",
       "  'would be as bright as the full moon. _That_ seems to me to be a bit',\n",
       "  'of propaganda on the part of opponents, or wishful thinking on the',\n",
       "  'part of proponents.',\n",
       "  '',\n",
       "  'Second, this charge of ruining the night sky permanently has been',\n",
       "  \"levelled against other projects, that either 1) don't increace light\",\n",
       "  'pollution significantly, or 2) increace light pollution only over the',\n",
       "  'target area.',\n",
       "  '',\n",
       "  'You may or may not recognize #1 as being Solar Power Sattelites.',\n",
       "  'I think it was Josh Hopkins who actually did the math, showing that',\n",
       "  \"SPS's weren't that bright after all, ending some two months of frenzied\",\n",
       "  'opposition on the part of dark-sky activists and various other types.',\n",
       "  '',\n",
       "  '#2 is mainly projects like the orbiting mirror the CIS tested',\n",
       "  \"recently.  While slightly more worrisome, I'd like to point out that\",\n",
       "  'any significant scattering of light outside the target area for one of',\n",
       "  'these mirrors would be wasted as far as the project would be',\n",
       "  'concerned, and something any project like that would work against',\n",
       "  \"anyway. And given some of the likely targets, I don't think there's\",\n",
       "  'going to be much of an outcry from the inhabitants. There is too much',\n",
       "  \"dark sky in the northern CIS during the winter, and I doubt you'll find\",\n",
       "  'many activists in Murmansk demanding the \"natural\" sky back. If anything,',\n",
       "  \"he'll probably be inside, stripped buck naked in front of the UV lamp,\",\n",
       "  'making sure he\\'ll get enough vitamin D for the \"day.\"',\n",
       "  '',\n",
       "  \"The mirror experiments aren't something they're doing for crass\",\n",
       "  \"advertising. They think that if they can build one, it'll be one of\",\n",
       "  \"those things people in the affected areas will think they couldn't\",\n",
       "  \"have lived without before. And I doubt anyone's going to really be\",\n",
       "  'able to convince them to stop.',\n",
       "  '',\n",
       "  '',\n",
       "  '--',\n",
       "  'Phil Fraering         |\"Number one good faith! You convert,',\n",
       "  'pgf@srl02.cacs.usl.edu|you not tortured by demons!\" - anon. Mahen missionary',\n",
       "  '',\n",
       "  '']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting list of lists into a flat_list\n",
    "lines = [] \n",
    "for sublist in l:\n",
    "    for item in sublist:\n",
    "        lines.append(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Newsgroups: comp.graphics',\n",
       " 'Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!uunet!mcsun!sun4nl!sun2.iend.wau.nl!PvdVeen.EL.WAU.NL',\n",
       " 'From: Peter.vanderveen@visser.el.wau.nl  (Peter van der Veen)',\n",
       " 'Subject: Re: Fonts in POV??',\n",
       " 'Message-ID: <19930415162925.Peter.vanderveen@PvdVeen.EL.WAU.NL>',\n",
       " 'Lines: 30',\n",
       " 'Sender: news@sun2.iend.wau.nl (Newsmanager WAU)',\n",
       " 'Organization: Wageningen Agricultural University',\n",
       " 'X-Newsreader: FTPNuz (DOS) v1.0',\n",
       " 'References: <1qg9fc$et9@wampyr.cc.uow.edu.au> ',\n",
       " 'Date: Thu, 15 Apr 1993 20:29:00 GMT',\n",
       " '',\n",
       " 'In Article <1qg9fc$et9@wampyr.cc.uow.edu.au> \"g9134255@wampyr.cc.uow.edu.au (Coronado Emmanuel Abad)\" says:',\n",
       " '> ',\n",
       " '> ',\n",
       " '> \\tI have seen several ray-traced scenes (from MTV or was it ',\n",
       " '> RayShade??) with stroked fonts appearing as objects in the image.',\n",
       " '> The fonts/chars had color, depth and even textures associated with',\n",
       " '> them.  Now I was wondering, is it possible to do the same in POV??',\n",
       " '> ',\n",
       " '> ',\n",
       " '> Thanks,',\n",
       " '> ',\n",
       " '> Noel',\n",
       " '> ',\n",
       " 'Yes, there are serveral programs which can convert font files (eq the Borland',\n",
       " 'fonts) to objects consisting of spheres, cones etc. ',\n",
       " \"I've used a program (forgot its name/place, but i can look for it) which\",\n",
       " 'converted these Borland fonts to three different raytracers. Vivid, POV and',\n",
       " 'Polyray (which i like more (more flexibel/faster/use of expressions etc).',\n",
       " 'The program has a lot nice features.',\n",
       " 'So if interested give me a mail.',\n",
       " '',\n",
       " ' /*---------*\\\\*/*-------------------------------------------*\\\\',\n",
       " ' *|  ____/|  *|*    PETER.VANDERVEEN@VISSER.EL.WAU.NL       |*',\n",
       " ' *|  \\\\ o.O|  *|*    Department of Genetics                  |*',\n",
       " ' *|   =(_)=  *|*    Agricultural University                 |*',\n",
       " ' *|     U    *|*    Wageningen, The Netherlands             |*',\n",
       " ' \\\\*---------*/*\\\\*-------------------------------------------*/',\n",
       " 'Xref: cantaloupe.srv.cs.cmu.edu rec.autos:102882 rec.autos.tech:54037 alt.autos.antique:2762 rec.autos.antique:1213',\n",
       " 'Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!ames!sun-barr!news2me.EBay.Sun.COM!jethro.Corp.Sun.COM!vavau!barnesm',\n",
       " 'From: barnesm@sun.com (Mark Barnes - SunSoft)',\n",
       " 'Newsgroups: rec.autos,rec.autos.tech,alt.autos.antique,rec.autos.antique',\n",
       " 'Subject: Re: WARNING.....(please read)...',\n",
       " 'Date: 15 Apr 1993 21:02:01 GMT',\n",
       " 'Organization: Sun Microsystems, Inc.',\n",
       " 'Lines: 12',\n",
       " 'Distribution: world',\n",
       " 'Message-ID: <1qkig9$mom@jethro.Corp.Sun.COM>',\n",
       " 'References: <1qke5b$mc4@spool.mu.edu>',\n",
       " 'Reply-To: barnesm@sun.com',\n",
       " 'NNTP-Posting-Host: vavau.corp.sun.com',\n",
       " '',\n",
       " '...and in San Francisco recently, some of our finest examples of humanity',\n",
       " 'poured oil over a road so that vehicles going uphill would suddnely become',\n",
       " 'immobile, and then they would walk right up to the vehicles and make their',\n",
       " 'demands known.',\n",
       " '--------------------------------+---------------------------------------',\n",
       " 'Mark Barnes, System Engineer    |  <insert standard disclaimers here>',\n",
       " 'SunSoft                         |',\n",
       " 'Corporate Technical Escalations |  I speak for myself, an individual,',\n",
       " 'Menlo Park, CA, USA             |  not for the company for which I work.',\n",
       " 'barnesm@vavau.Corp.Sun.COM      |',\n",
       " '--------------------------------+---------------------------------------',\n",
       " '',\n",
       " 'Xref: cantaloupe.srv.cs.cmu.edu sci.environment:30724 misc.consumers:69234 misc.invest:42646 sci.astro:36018 talk.environment:12219 talk.politics.space:2857 sci.space:62448 rec.backcountry:32284 misc.rural:6319 misc.headlines:42250',\n",
       " 'Newsgroups: sci.environment,misc.consumers,misc.invest,sci.astro,talk.environment,talk.politics.space,sci.space,rec.backcountry,misc.rural,misc.headlines,k12.chat.teacher',\n",
       " 'Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!darwin.sura.net!rouge!srl03.cacs.usl.edu!pgf',\n",
       " 'From: pgf@srl03.cacs.usl.edu (Phil G. Fraering)',\n",
       " 'Subject: Re: Space Marketing would be wonderfull.',\n",
       " 'Message-ID: <pgf.737604122@srl03.cacs.usl.edu>',\n",
       " 'Sender: anon@usl.edu (Anonymous NNTP Posting)',\n",
       " 'Organization: Univ. of Southwestern Louisiana',\n",
       " 'References: <FOX.93May15223005@graphics.nyu.edu> <C73rp4.3G2@lysator.liu.se> <16MAY93.11224501@meena.cc.uregina.ca> <pgf.737587842@srl03.cacs.usl.edu> <16MAY93.16432680@meena.cc.uregina.ca>',\n",
       " 'Date: Mon, 17 May 1993 02:02:02 GMT',\n",
       " 'Lines: 50',\n",
       " '',\n",
       " 'stange@meena.cc.uregina.ca writes:',\n",
       " '',\n",
       " '>Phil, your point is well taken.  It is still a sad idea.',\n",
       " '',\n",
       " \"I'm worried by the concern about it though, for a number of reasons\",\n",
       " 'that have nothing to do with Space Advertising (which for a number of',\n",
       " 'reasons is probably doomed to fail on financial grounds).',\n",
       " '',\n",
       " \"(And I've been reading and (and writing) this thread since way\",\n",
       " 'back when it was only on sci.space).',\n",
       " '',\n",
       " \"For starters, I don't think the piece of light-pollution apparatus\",\n",
       " 'would be as bright as the full moon. _That_ seems to me to be a bit',\n",
       " 'of propaganda on the part of opponents, or wishful thinking on the',\n",
       " 'part of proponents.',\n",
       " '',\n",
       " 'Second, this charge of ruining the night sky permanently has been',\n",
       " \"levelled against other projects, that either 1) don't increace light\",\n",
       " 'pollution significantly, or 2) increace light pollution only over the',\n",
       " 'target area.',\n",
       " '',\n",
       " 'You may or may not recognize #1 as being Solar Power Sattelites.',\n",
       " 'I think it was Josh Hopkins who actually did the math, showing that',\n",
       " \"SPS's weren't that bright after all, ending some two months of frenzied\",\n",
       " 'opposition on the part of dark-sky activists and various other types.',\n",
       " '',\n",
       " '#2 is mainly projects like the orbiting mirror the CIS tested',\n",
       " \"recently.  While slightly more worrisome, I'd like to point out that\",\n",
       " 'any significant scattering of light outside the target area for one of',\n",
       " 'these mirrors would be wasted as far as the project would be',\n",
       " 'concerned, and something any project like that would work against',\n",
       " \"anyway. And given some of the likely targets, I don't think there's\",\n",
       " 'going to be much of an outcry from the inhabitants. There is too much',\n",
       " \"dark sky in the northern CIS during the winter, and I doubt you'll find\",\n",
       " 'many activists in Murmansk demanding the \"natural\" sky back. If anything,',\n",
       " \"he'll probably be inside, stripped buck naked in front of the UV lamp,\",\n",
       " 'making sure he\\'ll get enough vitamin D for the \"day.\"',\n",
       " '',\n",
       " \"The mirror experiments aren't something they're doing for crass\",\n",
       " \"advertising. They think that if they can build one, it'll be one of\",\n",
       " \"those things people in the affected areas will think they couldn't\",\n",
       " \"have lived without before. And I doubt anyone's going to really be\",\n",
       " 'able to convince them to stop.',\n",
       " '',\n",
       " '',\n",
       " '--',\n",
       " 'Phil Fraering         |\"Number one good faith! You convert,',\n",
       " 'pgf@srl02.cacs.usl.edu|you not tortured by demons!\" - anon. Mahen missionary',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\devil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\devil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\devil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized words\n",
      "['newsgroups', 'comp', 'graphics']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['newsgroups', 'comp', 'graphics']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['newsgroups', 'comp', 'graphic']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['newsgroup', 'comp', 'graphic']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['newsgroup', 'comp', 'graphic']\n",
      "After pos_tags: [('newsgroup', 'JJ'), ('comp', 'NN'), ('graphic', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk newsgroup/JJ) (mychunk comp/NN graphic/NN))\n",
      "\n",
      "\n",
      "['newsgroup', 'comp', 'graphic']\n",
      "N_grams\n",
      "2-gram:  ['newsgroup comp', 'comp graphic']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'das', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'uunet', 'mcsun', 'sun', 'nl', 'sun', 'iend', 'wau', 'nl', 'pvdveen', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'das', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'uunet', 'mcsun', 'sun', 'nl', 'sun', 'iend', 'wau', 'nl', 'pvdveen', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['path', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'uunet', 'mcsun', 'sun', 'nl', 'sun', 'iend', 'wau', 'nl', 'pvdveen', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'uunet', 'mcsun', 'sun', 'nl', 'sun', 'iend', 'wau', 'nl', 'pvdveen', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'uunet', 'mcsun', 'sun', 'nl', 'sun', 'iend', 'wau', 'nl', 'pvdveen', 'el', 'wau', 'nl']\n",
      "After pos_tags: [('path', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('da', 'NN'), ('news', 'NN'), ('harvard', 'NN'), ('edu', 'NN'), ('noc', 'JJ'), ('near', 'IN'), ('net', 'JJ'), ('uunet', 'NN'), ('mcsun', 'NN'), ('sun', 'NN'), ('nl', 'NN'), ('sun', 'NN'), ('iend', 'VBP'), ('wau', 'NN'), ('nl', 'NN'), ('pvdveen', 'JJ'), ('el', 'NN'), ('wau', 'NN'), ('nl', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk path/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN da/NN news/NN harvard/NN edu/NN noc/JJ)\n",
      "  near/IN\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk uunet/NN mcsun/NN sun/NN nl/NN sun/NN)\n",
      "  iend/VBP\n",
      "  (mychunk wau/NN nl/NN pvdveen/JJ)\n",
      "  (mychunk el/NN wau/NN nl/NN))\n",
      "\n",
      "\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'uunet', 'mcsun', 'sun', 'nl', 'sun', 'iend', 'wau', 'nl', 'pvdveen', 'el', 'wau', 'nl']\n",
      "N_grams\n",
      "2-gram:  ['path cantaloup', 'cantaloup srv', 'srv c', 'c cmu', 'cmu edu', 'edu da', 'da news', 'news harvard', 'harvard edu', 'edu noc', 'noc near', 'near net', 'net uunet', 'uunet mcsun', 'mcsun sun', 'sun nl', 'nl sun', 'sun iend', 'iend wau', 'wau nl', 'nl pvdveen', 'pvdveen el', 'el wau', 'wau nl']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['from', 'peter', 'vanderveen', 'visser', 'el', 'wau', 'nl', 'peter', 'van', 'der', 'veen']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl', 'peter', 'van', 'der', 'veen']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl', 'peter', 'van', 'der', 'veen']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl', 'peter', 'van', 'der', 'veen']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl', 'peter', 'van', 'der', 'veen']\n",
      "After pos_tags: [('peter', 'NN'), ('vanderveen', 'NN'), ('visser', 'NN'), ('el', 'NN'), ('wau', 'NN'), ('nl', 'JJ'), ('peter', 'NN'), ('van', 'NN'), ('der', 'NN'), ('veen', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk peter/NN vanderveen/NN visser/NN el/NN wau/NN nl/JJ)\n",
      "  (mychunk peter/NN van/NN der/NN veen/NN))\n",
      "\n",
      "\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl', 'peter', 'van', 'der', 'veen']\n",
      "N_grams\n",
      "2-gram:  ['peter vanderveen', 'vanderveen visser', 'visser el', 'el wau', 'wau nl', 'nl peter', 'peter van', 'van der', 'der veen']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['subject', 're', 'fonts', 'in', 'pov']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['subject', 'fonts', 'pov']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['subject', 'font', 'pov']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['subject', 'font', 'pov']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['subject', 'font', 'pov']\n",
      "After pos_tags: [('subject', 'JJ'), ('font', 'NN'), ('pov', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk subject/JJ) (mychunk font/NN pov/NN))\n",
      "\n",
      "\n",
      "['subject', 'font', 'pov']\n",
      "N_grams\n",
      "2-gram:  ['subject font', 'font pov']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['message', 'id', 'peter', 'vanderveen', 'pvdveen', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['message', 'id', 'peter', 'vanderveen', 'pvdveen', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['message', 'id', 'peter', 'vanderveen', 'pvdveen', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['messag', 'id', 'peter', 'vanderveen', 'pvdveen', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['messag', 'id', 'peter', 'vanderveen', 'pvdveen', 'el', 'wau', 'nl']\n",
      "After pos_tags: [('messag', 'NN'), ('id', 'NN'), ('peter', 'NN'), ('vanderveen', 'NN'), ('pvdveen', 'JJ'), ('el', 'NN'), ('wau', 'NN'), ('nl', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk messag/NN id/NN peter/NN vanderveen/NN pvdveen/JJ)\n",
      "  (mychunk el/NN wau/NN nl/NN))\n",
      "\n",
      "\n",
      "['messag', 'id', 'peter', 'vanderveen', 'pvdveen', 'el', 'wau', 'nl']\n",
      "N_grams\n",
      "2-gram:  ['messag id', 'id peter', 'peter vanderveen', 'vanderveen pvdveen', 'pvdveen el', 'el wau', 'wau nl']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['lines']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['lines']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['line']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['line']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['line']\n",
      "After pos_tags: [('line', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk line/NN))\n",
      "\n",
      "\n",
      "['line']\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['sender', 'news', 'sun', 'iend', 'wau', 'nl', 'newsmanager', 'wau']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['sender', 'news', 'sun', 'iend', 'wau', 'nl', 'newsmanager', 'wau']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['sender', 'news', 'sun', 'iend', 'wau', 'nl', 'newsmanager', 'wau']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['sender', 'news', 'sun', 'iend', 'wau', 'nl', 'newsmanag', 'wau']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['sender', 'news', 'sun', 'iend', 'wau', 'nl', 'newsmanag', 'wau']\n",
      "After pos_tags: [('sender', 'NN'), ('news', 'NN'), ('sun', 'NN'), ('iend', 'VBP'), ('wau', 'NN'), ('nl', 'NN'), ('newsmanag', 'NN'), ('wau', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk sender/NN news/NN sun/NN)\n",
      "  iend/VBP\n",
      "  (mychunk wau/NN nl/NN newsmanag/NN wau/NN))\n",
      "\n",
      "\n",
      "['sender', 'news', 'sun', 'iend', 'wau', 'nl', 'newsmanag', 'wau']\n",
      "N_grams\n",
      "2-gram:  ['sender news', 'news sun', 'sun iend', 'iend wau', 'wau nl', 'nl newsmanag', 'newsmanag wau']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['organization', 'wageningen', 'agricultural', 'university']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['organization', 'wageningen', 'agricultural', 'university']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['organization', 'wageningen', 'agricultural', 'university']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['organ', 'wageningen', 'agricultur', 'univers']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['organ', 'wageningen', 'agricultur', 'univers']\n",
      "After pos_tags: [('organ', 'JJ'), ('wageningen', 'NN'), ('agricultur', 'NN'), ('univers', 'NNS')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk organ/JJ)\n",
      "  (mychunk wageningen/NN agricultur/NN univers/NNS))\n",
      "\n",
      "\n",
      "['organ', 'wageningen', 'agricultur', 'univers']\n",
      "N_grams\n",
      "2-gram:  ['organ wageningen', 'wageningen agricultur', 'agricultur univers']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['x', 'newsreader', 'ftpnuz', 'dos', 'v']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['x', 'newsreader', 'ftpnuz', 'dos', 'v']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['x', 'newsreader', 'ftpnuz', 'do', 'v']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['x', 'newsread', 'ftpnuz', 'do', 'v']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['x', 'newsread', 'ftpnuz', 'do', 'v']\n",
      "After pos_tags: [('x', 'JJ'), ('newsread', 'JJ'), ('ftpnuz', 'NN'), ('do', 'VBP'), ('v', 'NNS')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk x/JJ newsread/JJ)\n",
      "  (mychunk ftpnuz/NN)\n",
      "  do/VBP\n",
      "  (mychunk v/NNS))\n",
      "\n",
      "\n",
      "['x', 'newsread', 'ftpnuz', 'do', 'v']\n",
      "N_grams\n",
      "2-gram:  ['x newsread', 'newsread ftpnuz', 'ftpnuz do', 'do v']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['references', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['references', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reference', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['refer', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['refer', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au']\n",
      "After pos_tags: [('refer', 'NN'), ('qg', 'NN'), ('fc', 'NN'), ('et', 'FW'), ('wampyr', 'NN'), ('cc', 'NN'), ('uow', 'JJ'), ('edu', 'NN'), ('au', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk refer/NN qg/NN fc/NN)\n",
      "  et/FW\n",
      "  (mychunk wampyr/NN cc/NN uow/JJ)\n",
      "  (mychunk edu/NN au/NN))\n",
      "\n",
      "\n",
      "['refer', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au']\n",
      "N_grams\n",
      "2-gram:  ['refer qg', 'qg fc', 'fc et', 'et wampyr', 'wampyr cc', 'cc uow', 'uow edu', 'edu au']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['date', 'thu', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['date', 'thu', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['date', 'thu', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['date', 'thu', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['date', 'thu', 'apr', 'gmt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pos_tags: [('date', 'NN'), ('thu', 'NN'), ('apr', 'NN'), ('gmt', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk date/NN thu/NN apr/NN gmt/NN))\n",
      "\n",
      "\n",
      "['date', 'thu', 'apr', 'gmt']\n",
      "N_grams\n",
      "2-gram:  ['date thu', 'thu apr', 'apr gmt']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['in', 'article', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au', 'g', 'wampyr', 'cc', 'uow', 'edu', 'au', 'coronado', 'emmanuel', 'abad', 'says']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['article', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au', 'g', 'wampyr', 'cc', 'uow', 'edu', 'au', 'coronado', 'emmanuel', 'abad', 'says']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['article', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au', 'g', 'wampyr', 'cc', 'uow', 'edu', 'au', 'coronado', 'emmanuel', 'abad', 'say']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['articl', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au', 'g', 'wampyr', 'cc', 'uow', 'edu', 'au', 'coronado', 'emmanuel', 'abad', 'say']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['articl', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au', 'g', 'wampyr', 'cc', 'uow', 'edu', 'au', 'coronado', 'emmanuel', 'abad', 'say']\n",
      "After pos_tags: [('articl', 'NN'), ('qg', 'NN'), ('fc', 'NN'), ('et', 'FW'), ('wampyr', 'NN'), ('cc', 'NN'), ('uow', 'JJ'), ('edu', 'NN'), ('au', 'NN'), ('g', 'NN'), ('wampyr', 'NN'), ('cc', 'NN'), ('uow', 'JJ'), ('edu', 'NN'), ('au', 'NN'), ('coronado', 'NN'), ('emmanuel', 'NN'), ('abad', 'NNS'), ('say', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk articl/NN qg/NN fc/NN)\n",
      "  et/FW\n",
      "  (mychunk wampyr/NN cc/NN uow/JJ)\n",
      "  (mychunk edu/NN au/NN g/NN wampyr/NN cc/NN uow/JJ)\n",
      "  (mychunk edu/NN au/NN coronado/NN emmanuel/NN abad/NNS)\n",
      "  say/VBP)\n",
      "\n",
      "\n",
      "['articl', 'qg', 'fc', 'et', 'wampyr', 'cc', 'uow', 'edu', 'au', 'g', 'wampyr', 'cc', 'uow', 'edu', 'au', 'coronado', 'emmanuel', 'abad', 'say']\n",
      "N_grams\n",
      "2-gram:  ['articl qg', 'qg fc', 'fc et', 'et wampyr', 'wampyr cc', 'cc uow', 'uow edu', 'edu au', 'au g', 'g wampyr', 'wampyr cc', 'cc uow', 'uow edu', 'edu au', 'au coronado', 'coronado emmanuel', 'emmanuel abad', 'abad say']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['i', 'have', 'seen', 'several', 'ray', 'traced', 'scenes', 'from', 'mtv', 'or', 'was', 'it']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['seen', 'several', 'ray', 'traced', 'scenes', 'mtv']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['seen', 'several', 'ray', 'traced', 'scene', 'mtv']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['seen', 'sever', 'ray', 'trace', 'scene', 'mtv']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['seen', 'sever', 'ray', 'trace', 'scene', 'mtv']\n",
      "After pos_tags: [('seen', 'VBN'), ('sever', 'NN'), ('ray', 'NN'), ('trace', 'NN'), ('scene', 'NN'), ('mtv', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S seen/VBN (mychunk sever/NN ray/NN trace/NN scene/NN mtv/NN))\n",
      "\n",
      "\n",
      "['seen', 'sever', 'ray', 'trace', 'scene', 'mtv']\n",
      "N_grams\n",
      "2-gram:  ['seen sever', 'sever ray', 'ray trace', 'trace scene', 'scene mtv']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['rayshade', 'with', 'stroked', 'fonts', 'appearing', 'as', 'objects', 'in', 'the', 'image']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['rayshade', 'stroked', 'fonts', 'appearing', 'objects', 'image']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['rayshade', 'stroked', 'font', 'appearing', 'object', 'image']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['rayshad', 'stroke', 'font', 'appear', 'object', 'imag']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['rayshad', 'stroke', 'font', 'appear', 'object', 'imag']\n",
      "After pos_tags: [('rayshad', 'NNS'), ('stroke', 'VBD'), ('font', 'JJ'), ('appear', 'JJ'), ('object', 'NN'), ('imag', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk rayshad/NNS stroke/VBD font/JJ appear/JJ)\n",
      "  (mychunk object/NN imag/NN))\n",
      "\n",
      "\n",
      "['rayshad', 'stroke', 'font', 'appear', 'object', 'imag']\n",
      "N_grams\n",
      "2-gram:  ['rayshad stroke', 'stroke font', 'font appear', 'appear object', 'object imag']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['the', 'fonts', 'chars', 'had', 'color', 'depth', 'and', 'even', 'textures', 'associated', 'with']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['fonts', 'chars', 'color', 'depth', 'even', 'textures', 'associated']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['font', 'char', 'color', 'depth', 'even', 'texture', 'associated']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['font', 'char', 'color', 'depth', 'even', 'textur', 'associ']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['font', 'char', 'color', 'depth', 'even', 'textur', 'associ']\n",
      "After pos_tags: [('font', 'NN'), ('char', 'NN'), ('color', 'NN'), ('depth', 'NN'), ('even', 'RB'), ('textur', 'VBD'), ('associ', 'NNS')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk font/NN char/NN color/NN depth/NN)\n",
      "  even/RB\n",
      "  (mychunk textur/VBD)\n",
      "  (mychunk associ/NNS))\n",
      "\n",
      "\n",
      "['font', 'char', 'color', 'depth', 'even', 'textur', 'associ']\n",
      "N_grams\n",
      "2-gram:  ['font char', 'char color', 'color depth', 'depth even', 'even textur', 'textur associ']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['them', 'now', 'i', 'was', 'wondering', 'is', 'it', 'possible', 'to', 'do', 'the', 'same', 'in', 'pov']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['wondering', 'possible', 'pov']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['wondering', 'possible', 'pov']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['wonder', 'possibl', 'pov']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['wonder', 'possibl', 'pov']\n",
      "After pos_tags: [('wonder', 'NN'), ('possibl', 'NN'), ('pov', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk wonder/NN possibl/NN pov/NN))\n",
      "\n",
      "\n",
      "['wonder', 'possibl', 'pov']\n",
      "N_grams\n",
      "2-gram:  ['wonder possibl', 'possibl pov']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['thanks']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['thanks']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['thanks']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['thank']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['thank']\n",
      "After pos_tags: [('thank', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk thank/NN))\n",
      "\n",
      "\n",
      "['thank']\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['noel']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['noel']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['noel']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['noel']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['noel']\n",
      "After pos_tags: [('noel', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk noel/NN))\n",
      "\n",
      "\n",
      "['noel']\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['yes', 'there', 'are', 'serveral', 'programs', 'which', 'can', 'convert', 'font', 'files', 'eq', 'the', 'borland']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Stopwords\n",
      "['yes', 'serveral', 'programs', 'convert', 'font', 'files', 'eq', 'borland']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['yes', 'serveral', 'program', 'convert', 'font', 'file', 'eq', 'borland']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['ye', 'server', 'program', 'convert', 'font', 'file', 'eq', 'borland']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['ye', 'server', 'program', 'convert', 'font', 'file', 'eq', 'borland']\n",
      "After pos_tags: [('ye', 'JJ'), ('server', 'NN'), ('program', 'NN'), ('convert', 'NN'), ('font', 'NN'), ('file', 'NN'), ('eq', 'NN'), ('borland', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk ye/JJ)\n",
      "  (mychunk\n",
      "    server/NN\n",
      "    program/NN\n",
      "    convert/NN\n",
      "    font/NN\n",
      "    file/NN\n",
      "    eq/NN\n",
      "    borland/NN))\n",
      "\n",
      "\n",
      "['ye', 'server', 'program', 'convert', 'font', 'file', 'eq', 'borland']\n",
      "N_grams\n",
      "2-gram:  ['ye server', 'server program', 'program convert', 'convert font', 'font file', 'file eq', 'eq borland']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['fonts', 'to', 'objects', 'consisting', 'of', 'spheres', 'cones', 'etc']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['fonts', 'objects', 'consisting', 'spheres', 'cones', 'etc']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['font', 'object', 'consisting', 'sphere', 'cone', 'etc']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['font', 'object', 'consist', 'sphere', 'cone', 'etc']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['font', 'object', 'consist', 'sphere', 'cone', 'etc']\n",
      "After pos_tags: [('font', 'NN'), ('object', 'JJ'), ('consist', 'NN'), ('sphere', 'RB'), ('cone', 'CD'), ('etc', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk font/NN object/JJ)\n",
      "  (mychunk consist/NN)\n",
      "  sphere/RB\n",
      "  cone/CD\n",
      "  (mychunk etc/NN))\n",
      "\n",
      "\n",
      "['font', 'object', 'consist', 'sphere', 'cone', 'etc']\n",
      "N_grams\n",
      "2-gram:  ['font object', 'object consist', 'consist sphere', 'sphere cone', 'cone etc']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['i', 've', 'used', 'a', 'program', 'forgot', 'its', 'name', 'place', 'but', 'i', 'can', 'look', 'for', 'it', 'which']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['used', 'program', 'forgot', 'name', 'place', 'look']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['used', 'program', 'forgot', 'name', 'place', 'look']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['use', 'program', 'forgot', 'name', 'place', 'look']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['use', 'program', 'forgot', 'name', 'place', 'look']\n",
      "After pos_tags: [('use', 'NN'), ('program', 'NN'), ('forgot', 'VBD'), ('name', 'JJ'), ('place', 'NN'), ('look', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk use/NN program/NN forgot/VBD name/JJ)\n",
      "  (mychunk place/NN look/NN))\n",
      "\n",
      "\n",
      "['use', 'program', 'forgot', 'name', 'place', 'look']\n",
      "N_grams\n",
      "2-gram:  ['use program', 'program forgot', 'forgot name', 'name place', 'place look']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['converted', 'these', 'borland', 'fonts', 'to', 'three', 'different', 'raytracers', 'vivid', 'pov', 'and']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['converted', 'borland', 'fonts', 'three', 'different', 'raytracers', 'vivid', 'pov']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['converted', 'borland', 'font', 'three', 'different', 'raytracers', 'vivid', 'pov']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['convert', 'borland', 'font', 'three', 'differ', 'raytrac', 'vivid', 'pov']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['convert', 'borland', 'font', 'three', 'differ', 'raytrac', 'vivid', 'pov']\n",
      "After pos_tags: [('convert', 'NN'), ('borland', 'NN'), ('font', 'VBD'), ('three', 'CD'), ('differ', 'NN'), ('raytrac', 'NN'), ('vivid', 'NN'), ('pov', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk convert/NN borland/NN font/VBD)\n",
      "  three/CD\n",
      "  (mychunk differ/NN raytrac/NN vivid/NN pov/NN))\n",
      "\n",
      "\n",
      "['convert', 'borland', 'font', 'three', 'differ', 'raytrac', 'vivid', 'pov']\n",
      "N_grams\n",
      "2-gram:  ['convert borland', 'borland font', 'font three', 'three differ', 'differ raytrac', 'raytrac vivid', 'vivid pov']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['polyray', 'which', 'i', 'like', 'more', 'more', 'flexibel', 'faster', 'use', 'of', 'expressions', 'etc']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['polyray', 'like', 'flexibel', 'faster', 'use', 'expressions', 'etc']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['polyray', 'like', 'flexibel', 'faster', 'use', 'expression', 'etc']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['polyray', 'like', 'flexibel', 'faster', 'use', 'express', 'etc']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['polyray', 'like', 'flexibel', 'faster', 'use', 'express', 'etc']\n",
      "After pos_tags: [('polyray', 'NN'), ('like', 'IN'), ('flexibel', 'NN'), ('faster', 'RBR'), ('use', 'NN'), ('express', 'NN'), ('etc', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk polyray/NN)\n",
      "  like/IN\n",
      "  (mychunk flexibel/NN)\n",
      "  faster/RBR\n",
      "  (mychunk use/NN express/NN etc/NN))\n",
      "\n",
      "\n",
      "['polyray', 'like', 'flexibel', 'faster', 'use', 'express', 'etc']\n",
      "N_grams\n",
      "2-gram:  ['polyray like', 'like flexibel', 'flexibel faster', 'faster use', 'use express', 'express etc']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['the', 'program', 'has', 'a', 'lot', 'nice', 'features']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['program', 'lot', 'nice', 'features']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['program', 'lot', 'nice', 'feature']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['program', 'lot', 'nice', 'featur']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['program', 'lot', 'nice', 'featur']\n",
      "After pos_tags: [('program', 'NN'), ('lot', 'NN'), ('nice', 'JJ'), ('featur', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk program/NN lot/NN nice/JJ) (mychunk featur/NN))\n",
      "\n",
      "\n",
      "['program', 'lot', 'nice', 'featur']\n",
      "N_grams\n",
      "2-gram:  ['program lot', 'lot nice', 'nice featur']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['so', 'if', 'interested', 'give', 'me', 'a', 'mail']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['interested', 'give', 'mail']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['interested', 'give', 'mail']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['interest', 'give', 'mail']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['interest', 'give', 'mail']\n",
      "After pos_tags: [('interest', 'NN'), ('give', 'CD'), ('mail', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk interest/NN) give/CD (mychunk mail/NN))\n",
      "\n",
      "\n",
      "['interest', 'give', 'mail']\n",
      "N_grams\n",
      "2-gram:  ['interest give', 'give mail']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl']\n",
      "After pos_tags: [('peter', 'NN'), ('vanderveen', 'NN'), ('visser', 'NN'), ('el', 'NN'), ('wau', 'NN'), ('nl', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk peter/NN vanderveen/NN visser/NN el/NN wau/NN nl/NN))\n",
      "\n",
      "\n",
      "['peter', 'vanderveen', 'visser', 'el', 'wau', 'nl']\n",
      "N_grams\n",
      "2-gram:  ['peter vanderveen', 'vanderveen visser', 'visser el', 'el wau', 'wau nl']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['o', 'o', 'department', 'of', 'genetics']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['department', 'genetics']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['department', 'genetics']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['depart', 'genet']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['depart', 'genet']\n",
      "After pos_tags: [('depart', 'NN'), ('genet', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk depart/NN genet/NN))\n",
      "\n",
      "\n",
      "['depart', 'genet']\n",
      "N_grams\n",
      "2-gram:  ['depart genet']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['agricultural', 'university']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['agricultural', 'university']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['agricultural', 'university']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['agricultur', 'univers']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['agricultur', 'univers']\n",
      "After pos_tags: [('agricultur', 'NN'), ('univers', 'NNS')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk agricultur/NN univers/NNS))\n",
      "\n",
      "\n",
      "['agricultur', 'univers']\n",
      "N_grams\n",
      "2-gram:  ['agricultur univers']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['u', 'wageningen', 'the', 'netherlands']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['u', 'wageningen', 'netherlands']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['u', 'wageningen', 'netherlands']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['u', 'wageningen', 'netherland']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['u', 'wageningen', 'netherland']\n",
      "After pos_tags: [('u', 'JJ'), ('wageningen', 'NN'), ('netherland', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk u/JJ) (mychunk wageningen/NN netherland/NN))\n",
      "\n",
      "\n",
      "['u', 'wageningen', 'netherland']\n",
      "N_grams\n",
      "2-gram:  ['u wageningen', 'wageningen netherland']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'rec', 'autos', 'rec', 'autos', 'tech', 'alt', 'autos', 'antique', 'rec', 'autos', 'antique']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Stopwords\n",
      "['xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'rec', 'autos', 'rec', 'autos', 'tech', 'alt', 'autos', 'antique', 'rec', 'autos', 'antique']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['xref', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'rec', 'auto', 'rec', 'auto', 'tech', 'alt', 'auto', 'antique', 'rec', 'auto', 'antique']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rec', 'auto', 'rec', 'auto', 'tech', 'alt', 'auto', 'antiqu', 'rec', 'auto', 'antiqu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rec', 'auto', 'rec', 'auto', 'tech', 'alt', 'auto', 'antiqu', 'rec', 'auto', 'antiqu']\n",
      "After pos_tags: [('xref', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('rec', 'JJ'), ('auto', 'NN'), ('rec', 'JJ'), ('auto', 'NN'), ('tech', 'NN'), ('alt', 'JJ'), ('auto', 'NN'), ('antiqu', 'NN'), ('rec', 'JJ'), ('auto', 'NN'), ('antiqu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk xref/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN rec/JJ)\n",
      "  (mychunk auto/NN rec/JJ)\n",
      "  (mychunk auto/NN tech/NN alt/JJ)\n",
      "  (mychunk auto/NN antiqu/NN rec/JJ)\n",
      "  (mychunk auto/NN antiqu/NN))\n",
      "\n",
      "\n",
      "['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rec', 'auto', 'rec', 'auto', 'tech', 'alt', 'auto', 'antiqu', 'rec', 'auto', 'antiqu']\n",
      "N_grams\n",
      "2-gram:  ['xref cantaloup', 'cantaloup srv', 'srv c', 'c cmu', 'cmu edu', 'edu rec', 'rec auto', 'auto rec', 'rec auto', 'auto tech', 'tech alt', 'alt auto', 'auto antiqu', 'antiqu rec', 'rec auto', 'auto antiqu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'das', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'ans', 'net', 'agate', 'ames', 'sun', 'barr', 'news', 'me', 'ebay', 'sun', 'com', 'jethro', 'corp', 'sun', 'com', 'vavau', 'barnesm']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'das', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'ans', 'net', 'agate', 'ames', 'sun', 'barr', 'news', 'ebay', 'sun', 'com', 'jethro', 'corp', 'sun', 'com', 'vavau', 'barnesm']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['path', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'an', 'net', 'agate', 'ames', 'sun', 'barr', 'news', 'ebay', 'sun', 'com', 'jethro', 'corp', 'sun', 'com', 'vavau', 'barnesm']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'an', 'net', 'agat', 'ame', 'sun', 'barr', 'news', 'ebay', 'sun', 'com', 'jethro', 'corp', 'sun', 'com', 'vavau', 'barnesm']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'an', 'net', 'agat', 'ame', 'sun', 'barr', 'news', 'ebay', 'sun', 'com', 'jethro', 'corp', 'sun', 'com', 'vavau', 'barnesm']\n",
      "After pos_tags: [('path', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('da', 'NN'), ('news', 'NN'), ('harvard', 'NN'), ('edu', 'NN'), ('noc', 'JJ'), ('near', 'IN'), ('net', 'JJ'), ('howland', 'NN'), ('reston', 'NN'), ('an', 'DT'), ('net', 'JJ'), ('agat', 'NN'), ('ame', 'NN'), ('sun', 'NN'), ('barr', 'NN'), ('news', 'NN'), ('ebay', 'NN'), ('sun', 'NN'), ('com', 'NN'), ('jethro', 'NN'), ('corp', 'NN'), ('sun', 'NN'), ('com', 'NN'), ('vavau', 'NN'), ('barnesm', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk path/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN da/NN news/NN harvard/NN edu/NN noc/JJ)\n",
      "  near/IN\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk howland/NN reston/NN)\n",
      "  an/DT\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk\n",
      "    agat/NN\n",
      "    ame/NN\n",
      "    sun/NN\n",
      "    barr/NN\n",
      "    news/NN\n",
      "    ebay/NN\n",
      "    sun/NN\n",
      "    com/NN\n",
      "    jethro/NN\n",
      "    corp/NN\n",
      "    sun/NN\n",
      "    com/NN\n",
      "    vavau/NN\n",
      "    barnesm/NN))\n",
      "\n",
      "\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'an', 'net', 'agat', 'ame', 'sun', 'barr', 'news', 'ebay', 'sun', 'com', 'jethro', 'corp', 'sun', 'com', 'vavau', 'barnesm']\n",
      "N_grams\n",
      "2-gram:  ['path cantaloup', 'cantaloup srv', 'srv c', 'c cmu', 'cmu edu', 'edu da', 'da news', 'news harvard', 'harvard edu', 'edu noc', 'noc near', 'near net', 'net howland', 'howland reston', 'reston an', 'an net', 'net agat', 'agat ame', 'ame sun', 'sun barr', 'barr news', 'news ebay', 'ebay sun', 'sun com', 'com jethro', 'jethro corp', 'corp sun', 'sun com', 'com vavau', 'vavau barnesm']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['from', 'barnesm', 'sun', 'com', 'mark', 'barnes', 'sunsoft']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['barnesm', 'sun', 'com', 'mark', 'barnes', 'sunsoft']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['barnesm', 'sun', 'com', 'mark', 'barnes', 'sunsoft']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['barnesm', 'sun', 'com', 'mark', 'barn', 'sunsoft']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['barnesm', 'sun', 'com', 'mark', 'barn', 'sunsoft']\n",
      "After pos_tags: [('barnesm', 'NN'), ('sun', 'NN'), ('com', 'NN'), ('mark', 'NN'), ('barn', 'NN'), ('sunsoft', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk barnesm/NN sun/NN com/NN mark/NN barn/NN sunsoft/NN))\n",
      "\n",
      "\n",
      "['barnesm', 'sun', 'com', 'mark', 'barn', 'sunsoft']\n",
      "N_grams\n",
      "2-gram:  ['barnesm sun', 'sun com', 'com mark', 'mark barn', 'barn sunsoft']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['newsgroups', 'rec', 'autos', 'rec', 'autos', 'tech', 'alt', 'autos', 'antique', 'rec', 'autos', 'antique']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['newsgroups', 'rec', 'autos', 'rec', 'autos', 'tech', 'alt', 'autos', 'antique', 'rec', 'autos', 'antique']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['newsgroups', 'rec', 'auto', 'rec', 'auto', 'tech', 'alt', 'auto', 'antique', 'rec', 'auto', 'antique']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['newsgroup', 'rec', 'auto', 'rec', 'auto', 'tech', 'alt', 'auto', 'antiqu', 'rec', 'auto', 'antiqu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['newsgroup', 'rec', 'auto', 'rec', 'auto', 'tech', 'alt', 'auto', 'antiqu', 'rec', 'auto', 'antiqu']\n",
      "After pos_tags: [('newsgroup', 'NN'), ('rec', 'JJ'), ('auto', 'NN'), ('rec', 'JJ'), ('auto', 'NN'), ('tech', 'NN'), ('alt', 'JJ'), ('auto', 'NN'), ('antiqu', 'NN'), ('rec', 'JJ'), ('auto', 'NN'), ('antiqu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk newsgroup/NN rec/JJ)\n",
      "  (mychunk auto/NN rec/JJ)\n",
      "  (mychunk auto/NN tech/NN alt/JJ)\n",
      "  (mychunk auto/NN antiqu/NN rec/JJ)\n",
      "  (mychunk auto/NN antiqu/NN))\n",
      "\n",
      "\n",
      "['newsgroup', 'rec', 'auto', 'rec', 'auto', 'tech', 'alt', 'auto', 'antiqu', 'rec', 'auto', 'antiqu']\n",
      "N_grams\n",
      "2-gram:  ['newsgroup rec', 'rec auto', 'auto rec', 'rec auto', 'auto tech', 'tech alt', 'alt auto', 'auto antiqu', 'antiqu rec', 'rec auto', 'auto antiqu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['subject', 're', 'warning', 'please', 'read']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['subject', 'warning', 'please', 'read']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['subject', 'warning', 'please', 'read']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['subject', 'warn', 'pleas', 'read']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['subject', 'warn', 'pleas', 'read']\n",
      "After pos_tags: [('subject', 'JJ'), ('warn', 'NN'), ('pleas', 'NNS'), ('read', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk subject/JJ) (mychunk warn/NN pleas/NNS) read/VBP)\n",
      "\n",
      "\n",
      "['subject', 'warn', 'pleas', 'read']\n",
      "N_grams\n",
      "2-gram:  ['subject warn', 'warn pleas', 'pleas read']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['date', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['date', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['date', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['date', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['date', 'apr', 'gmt']\n",
      "After pos_tags: [('date', 'NN'), ('apr', 'NNS'), ('gmt', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk date/NN apr/NNS) gmt/VBP)\n",
      "\n",
      "\n",
      "['date', 'apr', 'gmt']\n",
      "N_grams\n",
      "2-gram:  ['date apr', 'apr gmt']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['organization', 'sun', 'microsystems', 'inc']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['organization', 'sun', 'microsystems', 'inc']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['organization', 'sun', 'microsystems', 'inc']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['organ', 'sun', 'microsystem', 'inc']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['organ', 'sun', 'microsystem', 'inc']\n",
      "After pos_tags: [('organ', 'JJ'), ('sun', 'NN'), ('microsystem', 'NN'), ('inc', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk organ/JJ) (mychunk sun/NN microsystem/NN inc/NN))\n",
      "\n",
      "\n",
      "['organ', 'sun', 'microsystem', 'inc']\n",
      "N_grams\n",
      "2-gram:  ['organ sun', 'sun microsystem', 'microsystem inc']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['lines']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['lines']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['line']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['line']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['line']\n",
      "After pos_tags: [('line', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk line/NN))\n",
      "\n",
      "\n",
      "['line']\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['distribut', 'world']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['distribut', 'world']\n",
      "After pos_tags: [('distribut', 'NN'), ('world', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk distribut/NN world/NN))\n",
      "\n",
      "\n",
      "['distribut', 'world']\n",
      "N_grams\n",
      "2-gram:  ['distribut world']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['message', 'id', 'qkig', 'mom', 'jethro', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['message', 'id', 'qkig', 'mom', 'jethro', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['message', 'id', 'qkig', 'mom', 'jethro', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['messag', 'id', 'qkig', 'mom', 'jethro', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['messag', 'id', 'qkig', 'mom', 'jethro', 'corp', 'sun', 'com']\n",
      "After pos_tags: [('messag', 'NN'), ('id', 'NN'), ('qkig', 'NN'), ('mom', 'NN'), ('jethro', 'NN'), ('corp', 'NN'), ('sun', 'NN'), ('com', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk\n",
      "    messag/NN\n",
      "    id/NN\n",
      "    qkig/NN\n",
      "    mom/NN\n",
      "    jethro/NN\n",
      "    corp/NN\n",
      "    sun/NN\n",
      "    com/NN))\n",
      "\n",
      "\n",
      "['messag', 'id', 'qkig', 'mom', 'jethro', 'corp', 'sun', 'com']\n",
      "N_grams\n",
      "2-gram:  ['messag id', 'id qkig', 'qkig mom', 'mom jethro', 'jethro corp', 'corp sun', 'sun com']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['references', 'qke', 'b', 'mc', 'spool', 'mu', 'edu']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['references', 'qke', 'b', 'mc', 'spool', 'mu', 'edu']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reference', 'qke', 'b', 'mc', 'spool', 'mu', 'edu']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['refer', 'qke', 'b', 'mc', 'spool', 'mu', 'edu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['refer', 'qke', 'b', 'mc', 'spool', 'mu', 'edu']\n",
      "After pos_tags: [('refer', 'NN'), ('qke', 'NN'), ('b', 'NN'), ('mc', 'NN'), ('spool', 'NN'), ('mu', 'NN'), ('edu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk refer/NN qke/NN b/NN mc/NN spool/NN mu/NN edu/NN))\n",
      "\n",
      "\n",
      "['refer', 'qke', 'b', 'mc', 'spool', 'mu', 'edu']\n",
      "N_grams\n",
      "2-gram:  ['refer qke', 'qke b', 'b mc', 'mc spool', 'spool mu', 'mu edu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['reply', 'to', 'barnesm', 'sun', 'com']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['reply', 'barnesm', 'sun', 'com']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reply', 'barnesm', 'sun', 'com']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['repli', 'barnesm', 'sun', 'com']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['repli', 'barnesm', 'sun', 'com']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pos_tags: [('repli', 'NN'), ('barnesm', 'NN'), ('sun', 'NN'), ('com', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk repli/NN barnesm/NN sun/NN com/NN))\n",
      "\n",
      "\n",
      "['repli', 'barnesm', 'sun', 'com']\n",
      "N_grams\n",
      "2-gram:  ['repli barnesm', 'barnesm sun', 'sun com']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['nntp', 'posting', 'host', 'vavau', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['nntp', 'posting', 'host', 'vavau', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['nntp', 'posting', 'host', 'vavau', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['nntp', 'post', 'host', 'vavau', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['nntp', 'post', 'host', 'vavau', 'corp', 'sun', 'com']\n",
      "After pos_tags: [('nntp', 'RB'), ('post', 'NN'), ('host', 'NN'), ('vavau', 'NN'), ('corp', 'NN'), ('sun', 'NN'), ('com', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S nntp/RB (mychunk post/NN host/NN vavau/NN corp/NN sun/NN com/NN))\n",
      "\n",
      "\n",
      "['nntp', 'post', 'host', 'vavau', 'corp', 'sun', 'com']\n",
      "N_grams\n",
      "2-gram:  ['nntp post', 'post host', 'host vavau', 'vavau corp', 'corp sun', 'sun com']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['and', 'in', 'san', 'francisco', 'recently', 'some', 'of', 'our', 'finest', 'examples', 'of', 'humanity']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['san', 'francisco', 'recently', 'finest', 'examples', 'humanity']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['san', 'francisco', 'recently', 'finest', 'example', 'humanity']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['san', 'francisco', 'recent', 'finest', 'exampl', 'human']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['san', 'francisco', 'recent', 'finest', 'exampl', 'human']\n",
      "After pos_tags: [('san', 'JJ'), ('francisco', 'JJ'), ('recent', 'JJ'), ('finest', 'JJS'), ('exampl', 'NN'), ('human', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk san/JJ francisco/JJ recent/JJ finest/JJS)\n",
      "  (mychunk exampl/NN human/NN))\n",
      "\n",
      "\n",
      "['san', 'francisco', 'recent', 'finest', 'exampl', 'human']\n",
      "N_grams\n",
      "2-gram:  ['san francisco', 'francisco recent', 'recent finest', 'finest exampl', 'exampl human']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['poured', 'oil', 'over', 'a', 'road', 'so', 'that', 'vehicles', 'going', 'uphill', 'would', 'suddnely', 'become']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['poured', 'oil', 'road', 'vehicles', 'going', 'uphill', 'would', 'suddnely', 'become']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['poured', 'oil', 'road', 'vehicle', 'going', 'uphill', 'would', 'suddnely', 'become']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['pour', 'oil', 'road', 'vehicl', 'go', 'uphil', 'would', 'suddn', 'becom']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['pour', 'oil', 'road', 'vehicl', 'go', 'uphil', 'would', 'suddn', 'becom']\n",
      "After pos_tags: [('pour', 'JJ'), ('oil', 'NN'), ('road', 'NN'), ('vehicl', 'NN'), ('go', 'VBP'), ('uphil', 'RB'), ('would', 'MD'), ('suddn', 'VB'), ('becom', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk pour/JJ)\n",
      "  (mychunk oil/NN road/NN vehicl/NN)\n",
      "  go/VBP\n",
      "  uphil/RB\n",
      "  would/MD\n",
      "  suddn/VB\n",
      "  (mychunk becom/NN))\n",
      "\n",
      "\n",
      "['pour', 'oil', 'road', 'vehicl', 'go', 'uphil', 'would', 'suddn', 'becom']\n",
      "N_grams\n",
      "2-gram:  ['pour oil', 'oil road', 'road vehicl', 'vehicl go', 'go uphil', 'uphil would', 'would suddn', 'suddn becom']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['immobile', 'and', 'then', 'they', 'would', 'walk', 'right', 'up', 'to', 'the', 'vehicles', 'and', 'make', 'their']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['immobile', 'would', 'walk', 'right', 'vehicles', 'make']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['immobile', 'would', 'walk', 'right', 'vehicle', 'make']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['immobil', 'would', 'walk', 'right', 'vehicl', 'make']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['immobil', 'would', 'walk', 'right', 'vehicl', 'make']\n",
      "After pos_tags: [('immobil', 'NN'), ('would', 'MD'), ('walk', 'VB'), ('right', 'JJ'), ('vehicl', 'NNS'), ('make', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk immobil/NN)\n",
      "  would/MD\n",
      "  walk/VB\n",
      "  (mychunk right/JJ)\n",
      "  (mychunk vehicl/NNS)\n",
      "  make/VBP)\n",
      "\n",
      "\n",
      "['immobil', 'would', 'walk', 'right', 'vehicl', 'make']\n",
      "N_grams\n",
      "2-gram:  ['immobil would', 'would walk', 'walk right', 'right vehicl', 'vehicl make']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['demands', 'known']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['demands', 'known']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['demand', 'known']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['demand', 'known']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['demand', 'known']\n",
      "After pos_tags: [('demand', 'NN'), ('known', 'VBN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk demand/NN) known/VBN)\n",
      "\n",
      "\n",
      "['demand', 'known']\n",
      "N_grams\n",
      "2-gram:  ['demand known']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['mark', 'barnes', 'system', 'engineer', 'insert', 'standard', 'disclaimers', 'here']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['mark', 'barnes', 'system', 'engineer', 'insert', 'standard', 'disclaimers']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['mark', 'barnes', 'system', 'engineer', 'insert', 'standard', 'disclaimer']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['mark', 'barn', 'system', 'engin', 'insert', 'standard', 'disclaim']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['mark', 'barn', 'system', 'engin', 'insert', 'standard', 'disclaim']\n",
      "After pos_tags: [('mark', 'NN'), ('barn', 'NN'), ('system', 'NN'), ('engin', 'VBP'), ('insert', 'JJ'), ('standard', 'NN'), ('disclaim', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk mark/NN barn/NN system/NN)\n",
      "  engin/VBP\n",
      "  (mychunk insert/JJ)\n",
      "  (mychunk standard/NN disclaim/NN))\n",
      "\n",
      "\n",
      "['mark', 'barn', 'system', 'engin', 'insert', 'standard', 'disclaim']\n",
      "N_grams\n",
      "2-gram:  ['mark barn', 'barn system', 'system engin', 'engin insert', 'insert standard', 'standard disclaim']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['sunsoft']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['sunsoft']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['sunsoft']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['sunsoft']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['sunsoft']\n",
      "After pos_tags: [('sunsoft', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk sunsoft/NN))\n",
      "\n",
      "\n",
      "['sunsoft']\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['corporate', 'technical', 'escalations', 'i', 'speak', 'for', 'myself', 'an', 'individual']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['corporate', 'technical', 'escalations', 'speak', 'individual']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['corporate', 'technical', 'escalation', 'speak', 'individual']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['corpor', 'technic', 'escal', 'speak', 'individu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['corpor', 'technic', 'escal', 'speak', 'individu']\n",
      "After pos_tags: [('corpor', 'NN'), ('technic', 'JJ'), ('escal', 'JJ'), ('speak', 'NN'), ('individu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk corpor/NN technic/JJ escal/JJ)\n",
      "  (mychunk speak/NN individu/NN))\n",
      "\n",
      "\n",
      "['corpor', 'technic', 'escal', 'speak', 'individu']\n",
      "N_grams\n",
      "2-gram:  ['corpor technic', 'technic escal', 'escal speak', 'speak individu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['menlo', 'park', 'ca', 'usa', 'not', 'for', 'the', 'company', 'for', 'which', 'i', 'work']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['menlo', 'park', 'ca', 'usa', 'company', 'work']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['menlo', 'park', 'ca', 'usa', 'company', 'work']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['menlo', 'park', 'ca', 'usa', 'compani', 'work']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['menlo', 'park', 'ca', 'usa', 'compani', 'work']\n",
      "After pos_tags: [('menlo', 'NN'), ('park', 'NN'), ('ca', 'MD'), ('usa', 'VB'), ('compani', 'NN'), ('work', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk menlo/NN park/NN)\n",
      "  ca/MD\n",
      "  usa/VB\n",
      "  (mychunk compani/NN work/NN))\n",
      "\n",
      "\n",
      "['menlo', 'park', 'ca', 'usa', 'compani', 'work']\n",
      "N_grams\n",
      "2-gram:  ['menlo park', 'park ca', 'ca usa', 'usa compani', 'compani work']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['barnesm', 'vavau', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['barnesm', 'vavau', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['barnesm', 'vavau', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['barnesm', 'vavau', 'corp', 'sun', 'com']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['barnesm', 'vavau', 'corp', 'sun', 'com']\n",
      "After pos_tags: [('barnesm', 'NN'), ('vavau', 'NN'), ('corp', 'NN'), ('sun', 'NN'), ('com', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk barnesm/NN vavau/NN corp/NN sun/NN com/NN))\n",
      "\n",
      "\n",
      "['barnesm', 'vavau', 'corp', 'sun', 'com']\n",
      "N_grams\n",
      "2-gram:  ['barnesm vavau', 'vavau corp', 'corp sun', 'sun com']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'sci', 'environment', 'misc', 'consumers', 'misc', 'invest', 'sci', 'astro', 'talk', 'environment', 'talk', 'politics', 'space', 'sci', 'space', 'rec', 'backcountry', 'misc', 'rural', 'misc', 'headlines']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'sci', 'environment', 'misc', 'consumers', 'misc', 'invest', 'sci', 'astro', 'talk', 'environment', 'talk', 'politics', 'space', 'sci', 'space', 'rec', 'backcountry', 'misc', 'rural', 'misc', 'headlines']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['xref', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'sci', 'environment', 'misc', 'consumer', 'misc', 'invest', 'sci', 'astro', 'talk', 'environment', 'talk', 'politics', 'space', 'sci', 'space', 'rec', 'backcountry', 'misc', 'rural', 'misc', 'headline']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'sci', 'environ', 'misc', 'consum', 'misc', 'invest', 'sci', 'astro', 'talk', 'environ', 'talk', 'polit', 'space', 'sci', 'space', 'rec', 'backcountri', 'misc', 'rural', 'misc', 'headlin']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'sci', 'environ', 'misc', 'consum', 'misc', 'invest', 'sci', 'astro', 'talk', 'environ', 'talk', 'polit', 'space', 'sci', 'space', 'rec', 'backcountri', 'misc', 'rural', 'misc', 'headlin']\n",
      "After pos_tags: [('xref', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('sci', 'NN'), ('environ', 'NNP'), ('misc', 'NN'), ('consum', 'NN'), ('misc', 'NN'), ('invest', 'VBP'), ('sci', 'NN'), ('astro', 'JJ'), ('talk', 'NN'), ('environ', 'NN'), ('talk', 'NN'), ('polit', 'VBD'), ('space', 'NN'), ('sci', 'NN'), ('space', 'NN'), ('rec', 'NN'), ('backcountri', 'NN'), ('misc', 'VBZ'), ('rural', 'JJ'), ('misc', 'NNS'), ('headlin', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk xref/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk\n",
      "    cmu/NN\n",
      "    edu/NN\n",
      "    sci/NN\n",
      "    environ/NNP\n",
      "    misc/NN\n",
      "    consum/NN\n",
      "    misc/NN)\n",
      "  invest/VBP\n",
      "  (mychunk sci/NN astro/JJ)\n",
      "  (mychunk talk/NN environ/NN talk/NN polit/VBD)\n",
      "  (mychunk space/NN sci/NN space/NN rec/NN backcountri/NN)\n",
      "  misc/VBZ\n",
      "  (mychunk rural/JJ)\n",
      "  (mychunk misc/NNS headlin/NN))\n",
      "\n",
      "\n",
      "['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'sci', 'environ', 'misc', 'consum', 'misc', 'invest', 'sci', 'astro', 'talk', 'environ', 'talk', 'polit', 'space', 'sci', 'space', 'rec', 'backcountri', 'misc', 'rural', 'misc', 'headlin']\n",
      "N_grams\n",
      "2-gram:  ['xref cantaloup', 'cantaloup srv', 'srv c', 'c cmu', 'cmu edu', 'edu sci', 'sci environ', 'environ misc', 'misc consum', 'consum misc', 'misc invest', 'invest sci', 'sci astro', 'astro talk', 'talk environ', 'environ talk', 'talk polit', 'polit space', 'space sci', 'sci space', 'space rec', 'rec backcountri', 'backcountri misc', 'misc rural', 'rural misc', 'misc headlin']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['newsgroups', 'sci', 'environment', 'misc', 'consumers', 'misc', 'invest', 'sci', 'astro', 'talk', 'environment', 'talk', 'politics', 'space', 'sci', 'space', 'rec', 'backcountry', 'misc', 'rural', 'misc', 'headlines', 'k', 'chat', 'teacher']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['newsgroups', 'sci', 'environment', 'misc', 'consumers', 'misc', 'invest', 'sci', 'astro', 'talk', 'environment', 'talk', 'politics', 'space', 'sci', 'space', 'rec', 'backcountry', 'misc', 'rural', 'misc', 'headlines', 'k', 'chat', 'teacher']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['newsgroups', 'sci', 'environment', 'misc', 'consumer', 'misc', 'invest', 'sci', 'astro', 'talk', 'environment', 'talk', 'politics', 'space', 'sci', 'space', 'rec', 'backcountry', 'misc', 'rural', 'misc', 'headline', 'k', 'chat', 'teacher']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['newsgroup', 'sci', 'environ', 'misc', 'consum', 'misc', 'invest', 'sci', 'astro', 'talk', 'environ', 'talk', 'polit', 'space', 'sci', 'space', 'rec', 'backcountri', 'misc', 'rural', 'misc', 'headlin', 'k', 'chat', 'teacher']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['newsgroup', 'sci', 'environ', 'misc', 'consum', 'misc', 'invest', 'sci', 'astro', 'talk', 'environ', 'talk', 'polit', 'space', 'sci', 'space', 'rec', 'backcountri', 'misc', 'rural', 'misc', 'headlin', 'k', 'chat', 'teacher']\n",
      "After pos_tags: [('newsgroup', 'JJ'), ('sci', 'NN'), ('environ', 'NNP'), ('misc', 'NN'), ('consum', 'NN'), ('misc', 'NN'), ('invest', 'VBP'), ('sci', 'NN'), ('astro', 'JJ'), ('talk', 'NN'), ('environ', 'NN'), ('talk', 'NN'), ('polit', 'VBD'), ('space', 'NN'), ('sci', 'NN'), ('space', 'NN'), ('rec', 'NN'), ('backcountri', 'NN'), ('misc', 'VBZ'), ('rural', 'JJ'), ('misc', 'NN'), ('headlin', 'NN'), ('k', 'NN'), ('chat', 'WP'), ('teacher', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk newsgroup/JJ)\n",
      "  (mychunk sci/NN environ/NNP misc/NN consum/NN misc/NN)\n",
      "  invest/VBP\n",
      "  (mychunk sci/NN astro/JJ)\n",
      "  (mychunk talk/NN environ/NN talk/NN polit/VBD)\n",
      "  (mychunk space/NN sci/NN space/NN rec/NN backcountri/NN)\n",
      "  misc/VBZ\n",
      "  (mychunk rural/JJ)\n",
      "  (mychunk misc/NN headlin/NN k/NN)\n",
      "  chat/WP\n",
      "  (mychunk teacher/NN))\n",
      "\n",
      "\n",
      "['newsgroup', 'sci', 'environ', 'misc', 'consum', 'misc', 'invest', 'sci', 'astro', 'talk', 'environ', 'talk', 'polit', 'space', 'sci', 'space', 'rec', 'backcountri', 'misc', 'rural', 'misc', 'headlin', 'k', 'chat', 'teacher']\n",
      "N_grams\n",
      "2-gram:  ['newsgroup sci', 'sci environ', 'environ misc', 'misc consum', 'consum misc', 'misc invest', 'invest sci', 'sci astro', 'astro talk', 'talk environ', 'environ talk', 'talk polit', 'polit space', 'space sci', 'sci space', 'space rec', 'rec backcountri', 'backcountri misc', 'misc rural', 'rural misc', 'misc headlin', 'headlin k', 'k chat', 'chat teacher']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'das', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'ans', 'net', 'darwin', 'sura', 'net', 'rouge', 'srl', 'cacs', 'usl', 'edu', 'pgf']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Stopwords\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'das', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'ans', 'net', 'darwin', 'sura', 'net', 'rouge', 'srl', 'cacs', 'usl', 'edu', 'pgf']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['path', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'an', 'net', 'darwin', 'sura', 'net', 'rouge', 'srl', 'cacs', 'usl', 'edu', 'pgf']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'an', 'net', 'darwin', 'sura', 'net', 'roug', 'srl', 'cac', 'usl', 'edu', 'pgf']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'an', 'net', 'darwin', 'sura', 'net', 'roug', 'srl', 'cac', 'usl', 'edu', 'pgf']\n",
      "After pos_tags: [('path', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('da', 'NN'), ('news', 'NN'), ('harvard', 'NN'), ('edu', 'NN'), ('noc', 'JJ'), ('near', 'IN'), ('net', 'JJ'), ('howland', 'NN'), ('reston', 'NN'), ('an', 'DT'), ('net', 'JJ'), ('darwin', 'NN'), ('sura', 'NN'), ('net', 'NN'), ('roug', 'NN'), ('srl', 'NN'), ('cac', 'NN'), ('usl', 'JJ'), ('edu', 'NN'), ('pgf', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk path/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN da/NN news/NN harvard/NN edu/NN noc/JJ)\n",
      "  near/IN\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk howland/NN reston/NN)\n",
      "  an/DT\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk darwin/NN sura/NN net/NN roug/NN srl/NN cac/NN usl/JJ)\n",
      "  (mychunk edu/NN pgf/NN))\n",
      "\n",
      "\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'da', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'an', 'net', 'darwin', 'sura', 'net', 'roug', 'srl', 'cac', 'usl', 'edu', 'pgf']\n",
      "N_grams\n",
      "2-gram:  ['path cantaloup', 'cantaloup srv', 'srv c', 'c cmu', 'cmu edu', 'edu da', 'da news', 'news harvard', 'harvard edu', 'edu noc', 'noc near', 'near net', 'net howland', 'howland reston', 'reston an', 'an net', 'net darwin', 'darwin sura', 'sura net', 'net roug', 'roug srl', 'srl cac', 'cac usl', 'usl edu', 'edu pgf']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['from', 'pgf', 'srl', 'cacs', 'usl', 'edu', 'phil', 'g', 'fraering']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['pgf', 'srl', 'cacs', 'usl', 'edu', 'phil', 'g', 'fraering']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['pgf', 'srl', 'cacs', 'usl', 'edu', 'phil', 'g', 'fraering']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['pgf', 'srl', 'cac', 'usl', 'edu', 'phil', 'g', 'fraer']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['pgf', 'srl', 'cac', 'usl', 'edu', 'phil', 'g', 'fraer']\n",
      "After pos_tags: [('pgf', 'NN'), ('srl', 'NN'), ('cac', 'NN'), ('usl', 'JJ'), ('edu', 'NN'), ('phil', 'NN'), ('g', 'NN'), ('fraer', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk pgf/NN srl/NN cac/NN usl/JJ)\n",
      "  (mychunk edu/NN phil/NN g/NN fraer/NN))\n",
      "\n",
      "\n",
      "['pgf', 'srl', 'cac', 'usl', 'edu', 'phil', 'g', 'fraer']\n",
      "N_grams\n",
      "2-gram:  ['pgf srl', 'srl cac', 'cac usl', 'usl edu', 'edu phil', 'phil g', 'g fraer']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['subject', 're', 'space', 'marketing', 'would', 'be', 'wonderfull']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['subject', 'space', 'marketing', 'would', 'wonderfull']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['subject', 'space', 'marketing', 'would', 'wonderfull']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['subject', 'space', 'market', 'would', 'wonderful']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['subject', 'space', 'market', 'would', 'wonderful']\n",
      "After pos_tags: [('subject', 'JJ'), ('space', 'NN'), ('market', 'NN'), ('would', 'MD'), ('wonderful', 'VB')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk subject/JJ)\n",
      "  (mychunk space/NN market/NN)\n",
      "  would/MD\n",
      "  wonderful/VB)\n",
      "\n",
      "\n",
      "['subject', 'space', 'market', 'would', 'wonderful']\n",
      "N_grams\n",
      "2-gram:  ['subject space', 'space market', 'market would', 'would wonderful']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['message', 'id', 'pgf', 'srl', 'cacs', 'usl', 'edu']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['message', 'id', 'pgf', 'srl', 'cacs', 'usl', 'edu']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['message', 'id', 'pgf', 'srl', 'cacs', 'usl', 'edu']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['messag', 'id', 'pgf', 'srl', 'cac', 'usl', 'edu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['messag', 'id', 'pgf', 'srl', 'cac', 'usl', 'edu']\n",
      "After pos_tags: [('messag', 'NN'), ('id', 'NN'), ('pgf', 'NN'), ('srl', 'NN'), ('cac', 'NN'), ('usl', 'JJ'), ('edu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk messag/NN id/NN pgf/NN srl/NN cac/NN usl/JJ)\n",
      "  (mychunk edu/NN))\n",
      "\n",
      "\n",
      "['messag', 'id', 'pgf', 'srl', 'cac', 'usl', 'edu']\n",
      "N_grams\n",
      "2-gram:  ['messag id', 'id pgf', 'pgf srl', 'srl cac', 'cac usl', 'usl edu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['sender', 'anon', 'usl', 'edu', 'anonymous', 'nntp', 'posting']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['sender', 'anon', 'usl', 'edu', 'anonymous', 'nntp', 'posting']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['sender', 'anon', 'usl', 'edu', 'anonymous', 'nntp', 'posting']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['sender', 'anon', 'usl', 'edu', 'anonym', 'nntp', 'post']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['sender', 'anon', 'usl', 'edu', 'anonym', 'nntp', 'post']\n",
      "After pos_tags: [('sender', 'NN'), ('anon', 'NN'), ('usl', 'JJ'), ('edu', 'NN'), ('anonym', 'NN'), ('nntp', 'VBZ'), ('post', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk sender/NN anon/NN usl/JJ)\n",
      "  (mychunk edu/NN anonym/NN)\n",
      "  nntp/VBZ\n",
      "  (mychunk post/NN))\n",
      "\n",
      "\n",
      "['sender', 'anon', 'usl', 'edu', 'anonym', 'nntp', 'post']\n",
      "N_grams\n",
      "2-gram:  ['sender anon', 'anon usl', 'usl edu', 'edu anonym', 'anonym nntp', 'nntp post']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['organization', 'univ', 'of', 'southwestern', 'louisiana']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['organization', 'univ', 'southwestern', 'louisiana']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['organization', 'univ', 'southwestern', 'louisiana']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['organ', 'univ', 'southwestern', 'louisiana']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['organ', 'univ', 'southwestern', 'louisiana']\n",
      "After pos_tags: [('organ', 'JJ'), ('univ', 'JJ'), ('southwestern', 'JJ'), ('louisiana', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk organ/JJ univ/JJ southwestern/JJ) (mychunk louisiana/NN))\n",
      "\n",
      "\n",
      "['organ', 'univ', 'southwestern', 'louisiana']\n",
      "N_grams\n",
      "2-gram:  ['organ univ', 'univ southwestern', 'southwestern louisiana']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['references', 'fox', 'may', 'graphics', 'nyu', 'edu', 'c', 'rp', 'g', 'lysator', 'liu', 'se', 'may', 'meena', 'cc', 'uregina', 'ca', 'pgf', 'srl', 'cacs', 'usl', 'edu', 'may', 'meena', 'cc', 'uregina', 'ca']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['references', 'fox', 'may', 'graphics', 'nyu', 'edu', 'c', 'rp', 'g', 'lysator', 'liu', 'se', 'may', 'meena', 'cc', 'uregina', 'ca', 'pgf', 'srl', 'cacs', 'usl', 'edu', 'may', 'meena', 'cc', 'uregina', 'ca']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reference', 'fox', 'may', 'graphic', 'nyu', 'edu', 'c', 'rp', 'g', 'lysator', 'liu', 'se', 'may', 'meena', 'cc', 'uregina', 'ca', 'pgf', 'srl', 'cacs', 'usl', 'edu', 'may', 'meena', 'cc', 'uregina', 'ca']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['refer', 'fox', 'may', 'graphic', 'nyu', 'edu', 'c', 'rp', 'g', 'lysat', 'liu', 'se', 'may', 'meena', 'cc', 'uregina', 'ca', 'pgf', 'srl', 'cac', 'usl', 'edu', 'may', 'meena', 'cc', 'uregina', 'ca']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['refer', 'fox', 'may', 'graphic', 'nyu', 'edu', 'c', 'rp', 'g', 'lysat', 'liu', 'se', 'may', 'meena', 'cc', 'uregina', 'ca', 'pgf', 'srl', 'cac', 'usl', 'edu', 'may', 'meena', 'cc', 'uregina', 'ca']\n",
      "After pos_tags: [('refer', 'NN'), ('fox', 'NN'), ('may', 'MD'), ('graphic', 'VB'), ('nyu', 'JJ'), ('edu', 'JJ'), ('c', 'NN'), ('rp', 'NN'), ('g', 'NN'), ('lysat', 'VBD'), ('liu', 'JJ'), ('se', 'NN'), ('may', 'MD'), ('meena', 'VB'), ('cc', 'NN'), ('uregina', 'JJ'), ('ca', 'MD'), ('pgf', 'VB'), ('srl', 'JJ'), ('cac', 'NN'), ('usl', 'JJ'), ('edu', 'NN'), ('may', 'MD'), ('meena', 'VB'), ('cc', 'NN'), ('uregina', 'NN'), ('ca', 'MD')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk refer/NN fox/NN)\n",
      "  may/MD\n",
      "  graphic/VB\n",
      "  (mychunk nyu/JJ edu/JJ)\n",
      "  (mychunk c/NN rp/NN g/NN lysat/VBD liu/JJ)\n",
      "  (mychunk se/NN)\n",
      "  may/MD\n",
      "  meena/VB\n",
      "  (mychunk cc/NN uregina/JJ)\n",
      "  ca/MD\n",
      "  pgf/VB\n",
      "  (mychunk srl/JJ)\n",
      "  (mychunk cac/NN usl/JJ)\n",
      "  (mychunk edu/NN)\n",
      "  may/MD\n",
      "  meena/VB\n",
      "  (mychunk cc/NN uregina/NN)\n",
      "  ca/MD)\n",
      "\n",
      "\n",
      "['refer', 'fox', 'may', 'graphic', 'nyu', 'edu', 'c', 'rp', 'g', 'lysat', 'liu', 'se', 'may', 'meena', 'cc', 'uregina', 'ca', 'pgf', 'srl', 'cac', 'usl', 'edu', 'may', 'meena', 'cc', 'uregina', 'ca']\n",
      "N_grams\n",
      "2-gram:  ['refer fox', 'fox may', 'may graphic', 'graphic nyu', 'nyu edu', 'edu c', 'c rp', 'rp g', 'g lysat', 'lysat liu', 'liu se', 'se may', 'may meena', 'meena cc', 'cc uregina', 'uregina ca', 'ca pgf', 'pgf srl', 'srl cac', 'cac usl', 'usl edu', 'edu may', 'may meena', 'meena cc', 'cc uregina', 'uregina ca']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['date', 'mon', 'may', 'gmt']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['date', 'mon', 'may', 'gmt']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['date', 'mon', 'may', 'gmt']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['date', 'mon', 'may', 'gmt']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['date', 'mon', 'may', 'gmt']\n",
      "After pos_tags: [('date', 'NN'), ('mon', 'NN'), ('may', 'MD'), ('gmt', 'VB')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk date/NN mon/NN) may/MD gmt/VB)\n",
      "\n",
      "\n",
      "['date', 'mon', 'may', 'gmt']\n",
      "N_grams\n",
      "2-gram:  ['date mon', 'mon may', 'may gmt']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['lines']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['lines']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['line']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['line']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['line']\n",
      "After pos_tags: [('line', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk line/NN))\n",
      "\n",
      "\n",
      "['line']\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['stange', 'meena', 'cc', 'uregina', 'ca', 'writes']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['stange', 'meena', 'cc', 'uregina', 'ca', 'writes']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['stange', 'meena', 'cc', 'uregina', 'ca', 'writes']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['stang', 'meena', 'cc', 'uregina', 'ca', 'write']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['stang', 'meena', 'cc', 'uregina', 'ca', 'write']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pos_tags: [('stang', 'NN'), ('meena', 'NN'), ('cc', 'NN'), ('uregina', 'NN'), ('ca', 'MD'), ('write', 'VB')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk stang/NN meena/NN cc/NN uregina/NN) ca/MD write/VB)\n",
      "\n",
      "\n",
      "['stang', 'meena', 'cc', 'uregina', 'ca', 'write']\n",
      "N_grams\n",
      "2-gram:  ['stang meena', 'meena cc', 'cc uregina', 'uregina ca', 'ca write']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['phil', 'your', 'point', 'is', 'well', 'taken', 'it', 'is', 'still', 'a', 'sad', 'idea']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['phil', 'point', 'well', 'taken', 'still', 'sad', 'idea']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['phil', 'point', 'well', 'taken', 'still', 'sad', 'idea']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['phil', 'point', 'well', 'taken', 'still', 'sad', 'idea']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['phil', 'point', 'well', 'taken', 'still', 'sad', 'idea']\n",
      "After pos_tags: [('phil', 'NN'), ('point', 'NN'), ('well', 'RB'), ('taken', 'VBN'), ('still', 'RB'), ('sad', 'JJ'), ('idea', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk phil/NN point/NN)\n",
      "  well/RB\n",
      "  taken/VBN\n",
      "  still/RB\n",
      "  (mychunk sad/JJ)\n",
      "  (mychunk idea/NN))\n",
      "\n",
      "\n",
      "['phil', 'point', 'well', 'taken', 'still', 'sad', 'idea']\n",
      "N_grams\n",
      "2-gram:  ['phil point', 'point well', 'well taken', 'taken still', 'still sad', 'sad idea']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['i', 'm', 'worried', 'by', 'the', 'concern', 'about', 'it', 'though', 'for', 'a', 'number', 'of', 'reasons']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['worried', 'concern', 'though', 'number', 'reasons']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['worried', 'concern', 'though', 'number', 'reason']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['worri', 'concern', 'though', 'number', 'reason']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['worri', 'concern', 'though', 'number', 'reason']\n",
      "After pos_tags: [('worri', 'NN'), ('concern', 'NN'), ('though', 'IN'), ('number', 'NN'), ('reason', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk worri/NN concern/NN)\n",
      "  though/IN\n",
      "  (mychunk number/NN reason/NN))\n",
      "\n",
      "\n",
      "['worri', 'concern', 'though', 'number', 'reason']\n",
      "N_grams\n",
      "2-gram:  ['worri concern', 'concern though', 'though number', 'number reason']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['that', 'have', 'nothing', 'to', 'do', 'with', 'space', 'advertising', 'which', 'for', 'a', 'number', 'of']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['nothing', 'space', 'advertising', 'number']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['nothing', 'space', 'advertising', 'number']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['noth', 'space', 'advertis', 'number']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['noth', 'space', 'advertis', 'number']\n",
      "After pos_tags: [('noth', 'DT'), ('space', 'NN'), ('advertis', 'NN'), ('number', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S noth/DT (mychunk space/NN advertis/NN number/NN))\n",
      "\n",
      "\n",
      "['noth', 'space', 'advertis', 'number']\n",
      "N_grams\n",
      "2-gram:  ['noth space', 'space advertis', 'advertis number']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['reasons', 'is', 'probably', 'doomed', 'to', 'fail', 'on', 'financial', 'grounds']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['reasons', 'probably', 'doomed', 'fail', 'financial', 'grounds']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reason', 'probably', 'doomed', 'fail', 'financial', 'ground']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['reason', 'probabl', 'doom', 'fail', 'financi', 'ground']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['reason', 'probabl', 'doom', 'fail', 'financi', 'ground']\n",
      "After pos_tags: [('reason', 'NN'), ('probabl', 'NN'), ('doom', 'NN'), ('fail', 'VBP'), ('financi', 'NN'), ('ground', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk reason/NN probabl/NN doom/NN)\n",
      "  fail/VBP\n",
      "  (mychunk financi/NN ground/NN))\n",
      "\n",
      "\n",
      "['reason', 'probabl', 'doom', 'fail', 'financi', 'ground']\n",
      "N_grams\n",
      "2-gram:  ['reason probabl', 'probabl doom', 'doom fail', 'fail financi', 'financi ground']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['and', 'i', 've', 'been', 'reading', 'and', 'and', 'writing', 'this', 'thread', 'since', 'way']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['reading', 'writing', 'thread', 'since', 'way']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reading', 'writing', 'thread', 'since', 'way']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['read', 'write', 'thread', 'sinc', 'way']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['read', 'write', 'thread', 'sinc', 'way']\n",
      "After pos_tags: [('read', 'VB'), ('write', 'JJ'), ('thread', 'JJ'), ('sinc', 'NN'), ('way', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S read/VB (mychunk write/JJ thread/JJ) (mychunk sinc/NN way/NN))\n",
      "\n",
      "\n",
      "['read', 'write', 'thread', 'sinc', 'way']\n",
      "N_grams\n",
      "2-gram:  ['read write', 'write thread', 'thread sinc', 'sinc way']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['back', 'when', 'it', 'was', 'only', 'on', 'sci', 'space']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['back', 'sci', 'space']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['back', 'sci', 'space']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['back', 'sci', 'space']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['back', 'sci', 'space']\n",
      "After pos_tags: [('back', 'RB'), ('sci', 'JJ'), ('space', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S back/RB (mychunk sci/JJ) (mychunk space/NN))\n",
      "\n",
      "\n",
      "['back', 'sci', 'space']\n",
      "N_grams\n",
      "2-gram:  ['back sci', 'sci space']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['for', 'starters', 'i', 'don', 't', 'think', 'the', 'piece', 'of', 'light', 'pollution', 'apparatus']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['starters', 'think', 'piece', 'light', 'pollution', 'apparatus']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['starter', 'think', 'piece', 'light', 'pollution', 'apparatus']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['starter', 'think', 'piec', 'light', 'pollut', 'apparatu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['starter', 'think', 'piec', 'light', 'pollut', 'apparatu']\n",
      "After pos_tags: [('starter', 'NN'), ('think', 'VBP'), ('piec', 'NN'), ('light', 'NN'), ('pollut', 'NN'), ('apparatu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk starter/NN)\n",
      "  think/VBP\n",
      "  (mychunk piec/NN light/NN pollut/NN apparatu/NN))\n",
      "\n",
      "\n",
      "['starter', 'think', 'piec', 'light', 'pollut', 'apparatu']\n",
      "N_grams\n",
      "2-gram:  ['starter think', 'think piec', 'piec light', 'light pollut', 'pollut apparatu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['would', 'be', 'as', 'bright', 'as', 'the', 'full', 'moon', 'that', 'seems', 'to', 'me', 'to', 'be', 'a', 'bit']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['would', 'bright', 'full', 'moon', 'seems', 'bit']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['would', 'bright', 'full', 'moon', 'seems', 'bit']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['would', 'bright', 'full', 'moon', 'seem', 'bit']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['would', 'bright', 'full', 'moon', 'seem', 'bit']\n",
      "After pos_tags: [('would', 'MD'), ('bright', 'VB'), ('full', 'JJ'), ('moon', 'NN'), ('seem', 'VBP'), ('bit', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  would/MD\n",
      "  bright/VB\n",
      "  (mychunk full/JJ)\n",
      "  (mychunk moon/NN)\n",
      "  seem/VBP\n",
      "  (mychunk bit/NN))\n",
      "\n",
      "\n",
      "['would', 'bright', 'full', 'moon', 'seem', 'bit']\n",
      "N_grams\n",
      "2-gram:  ['would bright', 'bright full', 'full moon', 'moon seem', 'seem bit']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['of', 'propaganda', 'on', 'the', 'part', 'of', 'opponents', 'or', 'wishful', 'thinking', 'on', 'the']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['propaganda', 'part', 'opponents', 'wishful', 'thinking']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['propaganda', 'part', 'opponent', 'wishful', 'thinking']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['propaganda', 'part', 'oppon', 'wish', 'think']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['propaganda', 'part', 'oppon', 'wish', 'think']\n",
      "After pos_tags: [('propaganda', 'JJ'), ('part', 'NN'), ('oppon', 'IN'), ('wish', 'JJ'), ('think', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk propaganda/JJ)\n",
      "  (mychunk part/NN)\n",
      "  oppon/IN\n",
      "  (mychunk wish/JJ)\n",
      "  (mychunk think/NN))\n",
      "\n",
      "\n",
      "['propaganda', 'part', 'oppon', 'wish', 'think']\n",
      "N_grams\n",
      "2-gram:  ['propaganda part', 'part oppon', 'oppon wish', 'wish think']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['part', 'of', 'proponents']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['part', 'proponents']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['part', 'proponent']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['part', 'propon']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['part', 'propon']\n",
      "After pos_tags: [('part', 'NN'), ('propon', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk part/NN propon/NN))\n",
      "\n",
      "\n",
      "['part', 'propon']\n",
      "N_grams\n",
      "2-gram:  ['part propon']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['second', 'this', 'charge', 'of', 'ruining', 'the', 'night', 'sky', 'permanently', 'has', 'been']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['second', 'charge', 'ruining', 'night', 'sky', 'permanently']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['second', 'charge', 'ruining', 'night', 'sky', 'permanently']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['second', 'charg', 'ruin', 'night', 'sky', 'perman']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['second', 'charg', 'ruin', 'night', 'sky', 'perman']\n",
      "After pos_tags: [('second', 'JJ'), ('charg', 'NN'), ('ruin', 'NN'), ('night', 'NN'), ('sky', 'NN'), ('perman', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk second/JJ)\n",
      "  (mychunk charg/NN ruin/NN night/NN sky/NN perman/NN))\n",
      "\n",
      "\n",
      "['second', 'charg', 'ruin', 'night', 'sky', 'perman']\n",
      "N_grams\n",
      "2-gram:  ['second charg', 'charg ruin', 'ruin night', 'night sky', 'sky perman']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['levelled', 'against', 'other', 'projects', 'that', 'either', 'don', 't', 'increace', 'light']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['levelled', 'projects', 'either', 'increace', 'light']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['levelled', 'project', 'either', 'increace', 'light']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['level', 'project', 'either', 'increac', 'light']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['level', 'project', 'either', 'increac', 'light']\n",
      "After pos_tags: [('level', 'NN'), ('project', 'NN'), ('either', 'DT'), ('increac', 'NN'), ('light', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk level/NN project/NN)\n",
      "  either/DT\n",
      "  (mychunk increac/NN light/NN))\n",
      "\n",
      "\n",
      "['level', 'project', 'either', 'increac', 'light']\n",
      "N_grams\n",
      "2-gram:  ['level project', 'project either', 'either increac', 'increac light']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['pollution', 'significantly', 'or', 'increace', 'light', 'pollution', 'only', 'over', 'the']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['pollution', 'significantly', 'increace', 'light', 'pollution']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['pollution', 'significantly', 'increace', 'light', 'pollution']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['pollut', 'significantli', 'increac', 'light', 'pollut']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['pollut', 'significantli', 'increac', 'light', 'pollut']\n",
      "After pos_tags: [('pollut', 'NN'), ('significantli', 'NN'), ('increac', 'NN'), ('light', 'VBD'), ('pollut', 'NNS')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk pollut/NN significantli/NN increac/NN light/VBD)\n",
      "  (mychunk pollut/NNS))\n",
      "\n",
      "\n",
      "['pollut', 'significantli', 'increac', 'light', 'pollut']\n",
      "N_grams\n",
      "2-gram:  ['pollut significantli', 'significantli increac', 'increac light', 'light pollut']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['target', 'area']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['target', 'area']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['target', 'area']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['target', 'area']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['target', 'area']\n",
      "After pos_tags: [('target', 'NN'), ('area', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk target/NN area/NN))\n",
      "\n",
      "\n",
      "['target', 'area']\n",
      "N_grams\n",
      "2-gram:  ['target area']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['you', 'may', 'or', 'may', 'not', 'recognize', 'as', 'being', 'solar', 'power', 'sattelites']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['may', 'may', 'recognize', 'solar', 'power', 'sattelites']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['may', 'may', 'recognize', 'solar', 'power', 'sattelites']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['may', 'may', 'recogn', 'solar', 'power', 'sattelit']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['may', 'may', 'recogn', 'solar', 'power', 'sattelit']\n",
      "After pos_tags: [('may', 'MD'), ('may', 'MD'), ('recogn', 'VB'), ('solar', 'JJ'), ('power', 'NN'), ('sattelit', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  may/MD\n",
      "  may/MD\n",
      "  recogn/VB\n",
      "  (mychunk solar/JJ)\n",
      "  (mychunk power/NN sattelit/NN))\n",
      "\n",
      "\n",
      "['may', 'may', 'recogn', 'solar', 'power', 'sattelit']\n",
      "N_grams\n",
      "2-gram:  ['may may', 'may recogn', 'recogn solar', 'solar power', 'power sattelit']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['i', 'think', 'it', 'was', 'josh', 'hopkins', 'who', 'actually', 'did', 'the', 'math', 'showing', 'that']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Stopwords\n",
      "['think', 'josh', 'hopkins', 'actually', 'math', 'showing']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['think', 'josh', 'hopkins', 'actually', 'math', 'showing']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['think', 'josh', 'hopkin', 'actual', 'math', 'show']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['think', 'josh', 'hopkin', 'actual', 'math', 'show']\n",
      "After pos_tags: [('think', 'VB'), ('josh', 'JJ'), ('hopkin', 'FW'), ('actual', 'JJ'), ('math', 'NN'), ('show', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  think/VB\n",
      "  (mychunk josh/JJ)\n",
      "  hopkin/FW\n",
      "  (mychunk actual/JJ)\n",
      "  (mychunk math/NN show/NN))\n",
      "\n",
      "\n",
      "['think', 'josh', 'hopkin', 'actual', 'math', 'show']\n",
      "N_grams\n",
      "2-gram:  ['think josh', 'josh hopkin', 'hopkin actual', 'actual math', 'math show']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['sps', 's', 'weren', 't', 'that', 'bright', 'after', 'all', 'ending', 'some', 'two', 'months', 'of', 'frenzied']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['sps', 'bright', 'ending', 'two', 'months', 'frenzied']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['sps', 'bright', 'ending', 'two', 'month', 'frenzied']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['sp', 'bright', 'end', 'two', 'month', 'frenzi']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['sp', 'bright', 'end', 'two', 'month', 'frenzi']\n",
      "After pos_tags: [('sp', 'NN'), ('bright', 'JJ'), ('end', 'NN'), ('two', 'CD'), ('month', 'NN'), ('frenzi', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk sp/NN bright/JJ)\n",
      "  (mychunk end/NN)\n",
      "  two/CD\n",
      "  (mychunk month/NN frenzi/NN))\n",
      "\n",
      "\n",
      "['sp', 'bright', 'end', 'two', 'month', 'frenzi']\n",
      "N_grams\n",
      "2-gram:  ['sp bright', 'bright end', 'end two', 'two month', 'month frenzi']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['opposition', 'on', 'the', 'part', 'of', 'dark', 'sky', 'activists', 'and', 'various', 'other', 'types']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['opposition', 'part', 'dark', 'sky', 'activists', 'various', 'types']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['opposition', 'part', 'dark', 'sky', 'activist', 'various', 'type']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['opposit', 'part', 'dark', 'sky', 'activist', 'variou', 'type']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['opposit', 'part', 'dark', 'sky', 'activist', 'variou', 'type']\n",
      "After pos_tags: [('opposit', 'IN'), ('part', 'NN'), ('dark', 'NN'), ('sky', 'JJ'), ('activist', 'NN'), ('variou', 'NN'), ('type', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  opposit/IN\n",
      "  (mychunk part/NN dark/NN sky/JJ)\n",
      "  (mychunk activist/NN variou/NN type/NN))\n",
      "\n",
      "\n",
      "['opposit', 'part', 'dark', 'sky', 'activist', 'variou', 'type']\n",
      "N_grams\n",
      "2-gram:  ['opposit part', 'part dark', 'dark sky', 'sky activist', 'activist variou', 'variou type']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['is', 'mainly', 'projects', 'like', 'the', 'orbiting', 'mirror', 'the', 'cis', 'tested']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['mainly', 'projects', 'like', 'orbiting', 'mirror', 'cis', 'tested']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['mainly', 'project', 'like', 'orbiting', 'mirror', 'ci', 'tested']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['mainli', 'project', 'like', 'orbit', 'mirror', 'ci', 'test']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['mainli', 'project', 'like', 'orbit', 'mirror', 'ci', 'test']\n",
      "After pos_tags: [('mainli', 'NN'), ('project', 'NN'), ('like', 'IN'), ('orbit', 'NN'), ('mirror', 'NN'), ('ci', 'NN'), ('test', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk mainli/NN project/NN)\n",
      "  like/IN\n",
      "  (mychunk orbit/NN mirror/NN ci/NN test/NN))\n",
      "\n",
      "\n",
      "['mainli', 'project', 'like', 'orbit', 'mirror', 'ci', 'test']\n",
      "N_grams\n",
      "2-gram:  ['mainli project', 'project like', 'like orbit', 'orbit mirror', 'mirror ci', 'ci test']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['recently', 'while', 'slightly', 'more', 'worrisome', 'i', 'd', 'like', 'to', 'point', 'out', 'that']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['recently', 'slightly', 'worrisome', 'like', 'point']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['recently', 'slightly', 'worrisome', 'like', 'point']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['recent', 'slightli', 'worrisom', 'like', 'point']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['recent', 'slightli', 'worrisom', 'like', 'point']\n",
      "After pos_tags: [('recent', 'JJ'), ('slightli', 'NN'), ('worrisom', 'NN'), ('like', 'IN'), ('point', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk recent/JJ)\n",
      "  (mychunk slightli/NN worrisom/NN)\n",
      "  like/IN\n",
      "  (mychunk point/NN))\n",
      "\n",
      "\n",
      "['recent', 'slightli', 'worrisom', 'like', 'point']\n",
      "N_grams\n",
      "2-gram:  ['recent slightli', 'slightli worrisom', 'worrisom like', 'like point']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['any', 'significant', 'scattering', 'of', 'light', 'outside', 'the', 'target', 'area', 'for', 'one', 'of']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['significant', 'scattering', 'light', 'outside', 'target', 'area', 'one']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['significant', 'scattering', 'light', 'outside', 'target', 'area', 'one']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['signific', 'scatter', 'light', 'outsid', 'target', 'area', 'one']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['signific', 'scatter', 'light', 'outsid', 'target', 'area', 'one']\n",
      "After pos_tags: [('signific', 'JJ'), ('scatter', 'NN'), ('light', 'NN'), ('outsid', 'NN'), ('target', 'NN'), ('area', 'NN'), ('one', 'CD')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk signific/JJ)\n",
      "  (mychunk scatter/NN light/NN outsid/NN target/NN area/NN)\n",
      "  one/CD)\n",
      "\n",
      "\n",
      "['signific', 'scatter', 'light', 'outsid', 'target', 'area', 'one']\n",
      "N_grams\n",
      "2-gram:  ['signific scatter', 'scatter light', 'light outsid', 'outsid target', 'target area', 'area one']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['these', 'mirrors', 'would', 'be', 'wasted', 'as', 'far', 'as', 'the', 'project', 'would', 'be']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['mirrors', 'would', 'wasted', 'far', 'project', 'would']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['mirror', 'would', 'wasted', 'far', 'project', 'would']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['mirror', 'would', 'wast', 'far', 'project', 'would']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['mirror', 'would', 'wast', 'far', 'project', 'would']\n",
      "After pos_tags: [('mirror', 'NN'), ('would', 'MD'), ('wast', 'VB'), ('far', 'RB'), ('project', 'NN'), ('would', 'MD')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk mirror/NN)\n",
      "  would/MD\n",
      "  wast/VB\n",
      "  far/RB\n",
      "  (mychunk project/NN)\n",
      "  would/MD)\n",
      "\n",
      "\n",
      "['mirror', 'would', 'wast', 'far', 'project', 'would']\n",
      "N_grams\n",
      "2-gram:  ['mirror would', 'would wast', 'wast far', 'far project', 'project would']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['concerned', 'and', 'something', 'any', 'project', 'like', 'that', 'would', 'work', 'against']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['concerned', 'something', 'project', 'like', 'would', 'work']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['concerned', 'something', 'project', 'like', 'would', 'work']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['concern', 'someth', 'project', 'like', 'would', 'work']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['concern', 'someth', 'project', 'like', 'would', 'work']\n",
      "After pos_tags: [('concern', 'NN'), ('someth', 'VBZ'), ('project', 'NN'), ('like', 'IN'), ('would', 'MD'), ('work', 'VB')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk concern/NN)\n",
      "  someth/VBZ\n",
      "  (mychunk project/NN)\n",
      "  like/IN\n",
      "  would/MD\n",
      "  work/VB)\n",
      "\n",
      "\n",
      "['concern', 'someth', 'project', 'like', 'would', 'work']\n",
      "N_grams\n",
      "2-gram:  ['concern someth', 'someth project', 'project like', 'like would', 'would work']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['anyway', 'and', 'given', 'some', 'of', 'the', 'likely', 'targets', 'i', 'don', 't', 'think', 'there', 's']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['anyway', 'given', 'likely', 'targets', 'think']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['anyway', 'given', 'likely', 'target', 'think']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming\n",
      "['anyway', 'given', 'like', 'target', 'think']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['anyway', 'given', 'like', 'target', 'think']\n",
      "After pos_tags: [('anyway', 'RB'), ('given', 'VBN'), ('like', 'IN'), ('target', 'NN'), ('think', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S anyway/RB given/VBN like/IN (mychunk target/NN) think/VBP)\n",
      "\n",
      "\n",
      "['anyway', 'given', 'like', 'target', 'think']\n",
      "N_grams\n",
      "2-gram:  ['anyway given', 'given like', 'like target', 'target think']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['going', 'to', 'be', 'much', 'of', 'an', 'outcry', 'from', 'the', 'inhabitants', 'there', 'is', 'too', 'much']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['going', 'much', 'outcry', 'inhabitants', 'much']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['going', 'much', 'outcry', 'inhabitant', 'much']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['go', 'much', 'outcri', 'inhabit', 'much']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['go', 'much', 'outcri', 'inhabit', 'much']\n",
      "After pos_tags: [('go', 'VB'), ('much', 'JJ'), ('outcri', 'NN'), ('inhabit', 'NN'), ('much', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S go/VB (mychunk much/JJ) (mychunk outcri/NN inhabit/NN much/JJ))\n",
      "\n",
      "\n",
      "['go', 'much', 'outcri', 'inhabit', 'much']\n",
      "N_grams\n",
      "2-gram:  ['go much', 'much outcri', 'outcri inhabit', 'inhabit much']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['dark', 'sky', 'in', 'the', 'northern', 'cis', 'during', 'the', 'winter', 'and', 'i', 'doubt', 'you', 'll', 'find']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['dark', 'sky', 'northern', 'cis', 'winter', 'doubt', 'find']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['dark', 'sky', 'northern', 'ci', 'winter', 'doubt', 'find']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['dark', 'sky', 'northern', 'ci', 'winter', 'doubt', 'find']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['dark', 'sky', 'northern', 'ci', 'winter', 'doubt', 'find']\n",
      "After pos_tags: [('dark', 'NN'), ('sky', 'NN'), ('northern', 'JJ'), ('ci', 'NN'), ('winter', 'NN'), ('doubt', 'NN'), ('find', 'VB')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk dark/NN sky/NN northern/JJ)\n",
      "  (mychunk ci/NN winter/NN doubt/NN)\n",
      "  find/VB)\n",
      "\n",
      "\n",
      "['dark', 'sky', 'northern', 'ci', 'winter', 'doubt', 'find']\n",
      "N_grams\n",
      "2-gram:  ['dark sky', 'sky northern', 'northern ci', 'ci winter', 'winter doubt', 'doubt find']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['many', 'activists', 'in', 'murmansk', 'demanding', 'the', 'natural', 'sky', 'back', 'if', 'anything']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['many', 'activists', 'murmansk', 'demanding', 'natural', 'sky', 'back', 'anything']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['many', 'activist', 'murmansk', 'demanding', 'natural', 'sky', 'back', 'anything']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['mani', 'activist', 'murmansk', 'demand', 'natur', 'sky', 'back', 'anyth']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['mani', 'activist', 'murmansk', 'demand', 'natur', 'sky', 'back', 'anyth']\n",
      "After pos_tags: [('mani', 'NNS'), ('activist', 'VBP'), ('murmansk', 'JJ'), ('demand', 'NN'), ('natur', 'NN'), ('sky', 'VBP'), ('back', 'RB'), ('anyth', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk mani/NNS)\n",
      "  activist/VBP\n",
      "  (mychunk murmansk/JJ)\n",
      "  (mychunk demand/NN natur/NN)\n",
      "  sky/VBP\n",
      "  back/RB\n",
      "  (mychunk anyth/NN))\n",
      "\n",
      "\n",
      "['mani', 'activist', 'murmansk', 'demand', 'natur', 'sky', 'back', 'anyth']\n",
      "N_grams\n",
      "2-gram:  ['mani activist', 'activist murmansk', 'murmansk demand', 'demand natur', 'natur sky', 'sky back', 'back anyth']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['he', 'll', 'probably', 'be', 'inside', 'stripped', 'buck', 'naked', 'in', 'front', 'of', 'the', 'uv', 'lamp']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['probably', 'inside', 'stripped', 'buck', 'naked', 'front', 'uv', 'lamp']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['probably', 'inside', 'stripped', 'buck', 'naked', 'front', 'uv', 'lamp']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['probabl', 'insid', 'strip', 'buck', 'nake', 'front', 'uv', 'lamp']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['probabl', 'insid', 'strip', 'buck', 'nake', 'front', 'uv', 'lamp']\n",
      "After pos_tags: [('probabl', 'NN'), ('insid', 'NN'), ('strip', 'NN'), ('buck', 'VBP'), ('nake', 'JJ'), ('front', 'NN'), ('uv', 'NN'), ('lamp', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk probabl/NN insid/NN strip/NN)\n",
      "  buck/VBP\n",
      "  (mychunk nake/JJ)\n",
      "  (mychunk front/NN uv/NN lamp/NN))\n",
      "\n",
      "\n",
      "['probabl', 'insid', 'strip', 'buck', 'nake', 'front', 'uv', 'lamp']\n",
      "N_grams\n",
      "2-gram:  ['probabl insid', 'insid strip', 'strip buck', 'buck nake', 'nake front', 'front uv', 'uv lamp']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['making', 'sure', 'he', 'll', 'get', 'enough', 'vitamin', 'd', 'for', 'the', 'day']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['making', 'sure', 'get', 'enough', 'vitamin', 'day']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['making', 'sure', 'get', 'enough', 'vitamin', 'day']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['make', 'sure', 'get', 'enough', 'vitamin', 'day']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['make', 'sure', 'get', 'enough', 'vitamin', 'day']\n",
      "After pos_tags: [('make', 'VB'), ('sure', 'JJ'), ('get', 'VB'), ('enough', 'JJ'), ('vitamin', 'NN'), ('day', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  make/VB\n",
      "  (mychunk sure/JJ)\n",
      "  get/VB\n",
      "  (mychunk enough/JJ)\n",
      "  (mychunk vitamin/NN day/NN))\n",
      "\n",
      "\n",
      "['make', 'sure', 'get', 'enough', 'vitamin', 'day']\n",
      "N_grams\n",
      "2-gram:  ['make sure', 'sure get', 'get enough', 'enough vitamin', 'vitamin day']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['the', 'mirror', 'experiments', 'aren', 't', 'something', 'they', 're', 'doing', 'for', 'crass']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['mirror', 'experiments', 'something', 'crass']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['mirror', 'experiment', 'something', 'crass']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['mirror', 'experi', 'someth', 'crass']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['mirror', 'experi', 'someth', 'crass']\n",
      "After pos_tags: [('mirror', 'NN'), ('experi', 'NN'), ('someth', 'VBZ'), ('crass', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk mirror/NN experi/NN) someth/VBZ (mychunk crass/NN))\n",
      "\n",
      "\n",
      "['mirror', 'experi', 'someth', 'crass']\n",
      "N_grams\n",
      "2-gram:  ['mirror experi', 'experi someth', 'someth crass']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['advertising', 'they', 'think', 'that', 'if', 'they', 'can', 'build', 'one', 'it', 'll', 'be', 'one', 'of']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['advertising', 'think', 'build', 'one', 'one']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['advertising', 'think', 'build', 'one', 'one']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['advertis', 'think', 'build', 'one', 'one']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['advertis', 'think', 'build', 'one', 'one']\n",
      "After pos_tags: [('advertis', 'NN'), ('think', 'VBP'), ('build', 'VB'), ('one', 'CD'), ('one', 'CD')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk advertis/NN) think/VBP build/VB one/CD one/CD)\n",
      "\n",
      "\n",
      "['advertis', 'think', 'build', 'one', 'one']\n",
      "N_grams\n",
      "2-gram:  ['advertis think', 'think build', 'build one', 'one one']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['those', 'things', 'people', 'in', 'the', 'affected', 'areas', 'will', 'think', 'they', 'couldn', 't']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['things', 'people', 'affected', 'areas', 'think']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['thing', 'people', 'affected', 'area', 'think']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['thing', 'peopl', 'affect', 'area', 'think']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['thing', 'peopl', 'affect', 'area', 'think']\n",
      "After pos_tags: [('thing', 'NN'), ('peopl', 'NN'), ('affect', 'VBP'), ('area', 'NN'), ('think', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk thing/NN peopl/NN)\n",
      "  affect/VBP\n",
      "  (mychunk area/NN)\n",
      "  think/VBP)\n",
      "\n",
      "\n",
      "['thing', 'peopl', 'affect', 'area', 'think']\n",
      "N_grams\n",
      "2-gram:  ['thing peopl', 'peopl affect', 'affect area', 'area think']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['have', 'lived', 'without', 'before', 'and', 'i', 'doubt', 'anyone', 's', 'going', 'to', 'really', 'be']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['lived', 'without', 'doubt', 'anyone', 'going', 'really']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['lived', 'without', 'doubt', 'anyone', 'going', 'really']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['live', 'without', 'doubt', 'anyon', 'go', 'realli']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['live', 'without', 'doubt', 'anyon', 'go', 'realli']\n",
      "After pos_tags: [('live', 'NN'), ('without', 'IN'), ('doubt', 'NN'), ('anyon', 'NN'), ('go', 'VBP'), ('realli', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk live/NN)\n",
      "  without/IN\n",
      "  (mychunk doubt/NN anyon/NN)\n",
      "  go/VBP\n",
      "  (mychunk realli/NN))\n",
      "\n",
      "\n",
      "['live', 'without', 'doubt', 'anyon', 'go', 'realli']\n",
      "N_grams\n",
      "2-gram:  ['live without', 'without doubt', 'doubt anyon', 'anyon go', 'go realli']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['able', 'to', 'convince', 'them', 'to', 'stop']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['able', 'convince', 'stop']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['able', 'convince', 'stop']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['abl', 'convinc', 'stop']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['abl', 'convinc', 'stop']\n",
      "After pos_tags: [('abl', 'NN'), ('convinc', 'NN'), ('stop', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk abl/NN convinc/NN stop/NN))\n",
      "\n",
      "\n",
      "['abl', 'convinc', 'stop']\n",
      "N_grams\n",
      "2-gram:  ['abl convinc', 'convinc stop']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['phil', 'fraering', 'number', 'one', 'good', 'faith', 'you', 'convert']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['phil', 'fraering', 'number', 'one', 'good', 'faith', 'convert']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['phil', 'fraering', 'number', 'one', 'good', 'faith', 'convert']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['phil', 'fraer', 'number', 'one', 'good', 'faith', 'convert']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['phil', 'fraer', 'number', 'one', 'good', 'faith', 'convert']\n",
      "After pos_tags: [('phil', 'NN'), ('fraer', 'NN'), ('number', 'NN'), ('one', 'CD'), ('good', 'JJ'), ('faith', 'NN'), ('convert', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk phil/NN fraer/NN number/NN)\n",
      "  one/CD\n",
      "  (mychunk good/JJ)\n",
      "  (mychunk faith/NN convert/NN))\n",
      "\n",
      "\n",
      "['phil', 'fraer', 'number', 'one', 'good', 'faith', 'convert']\n",
      "N_grams\n",
      "2-gram:  ['phil fraer', 'fraer number', 'number one', 'one good', 'good faith', 'faith convert']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['pgf', 'srl', 'cacs', 'usl', 'edu', 'you', 'not', 'tortured', 'by', 'demons', 'anon', 'mahen', 'missionary']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['pgf', 'srl', 'cacs', 'usl', 'edu', 'tortured', 'demons', 'anon', 'mahen', 'missionary']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['pgf', 'srl', 'cacs', 'usl', 'edu', 'tortured', 'demon', 'anon', 'mahen', 'missionary']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['pgf', 'srl', 'cac', 'usl', 'edu', 'tortur', 'demon', 'anon', 'mahen', 'missionari']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['pgf', 'srl', 'cac', 'usl', 'edu', 'tortur', 'demon', 'anon', 'mahen', 'missionari']\n",
      "After pos_tags: [('pgf', 'NN'), ('srl', 'NN'), ('cac', 'NN'), ('usl', 'JJ'), ('edu', 'NN'), ('tortur', 'NN'), ('demon', 'NN'), ('anon', 'NN'), ('mahen', 'NN'), ('missionari', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk pgf/NN srl/NN cac/NN usl/JJ)\n",
      "  (mychunk edu/NN tortur/NN demon/NN anon/NN mahen/NN missionari/NN))\n",
      "\n",
      "\n",
      "['pgf', 'srl', 'cac', 'usl', 'edu', 'tortur', 'demon', 'anon', 'mahen', 'missionari']\n",
      "N_grams\n",
      "2-gram:  ['pgf srl', 'srl cac', 'cac usl', 'usl edu', 'edu tortur', 'tortur demon', 'demon anon', 'anon mahen', 'mahen missionari']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(line, num):\n",
    "    n_grams = ngrams(line, num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk import word_tokenize, pos_tag\n",
    "for i in range(len(lines)):\n",
    "    line=[]\n",
    "    line=re.sub('[^a-zA-Z]',' ',lines[i])\n",
    "    line = line.lower()\n",
    "    line = line.split()#tokenization \n",
    "    print(\"tokenized words\")\n",
    "    print(line)\n",
    "    print(\"\\n\")\n",
    "    line=[word for word in line if word not in set(stopwords.words('english'))]\n",
    "    print(\"Removing Stopwords\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    line = [lemmatizer.lemmatize(word) for word in line]\n",
    "    print(\"Lemmatization\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    line = [ps.stem(word) for word in line]\n",
    "    print(\"Stemming\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    print(\"pos_tags and chunking\")\n",
    "    print(\"After Split:\",line)\n",
    "    tokens_tag = pos_tag(line)\n",
    "    print(\"After pos_tags:\",tokens_tag)\n",
    "    patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "    chunker = RegexpParser(patterns)\n",
    "    print(\"After Regex:\",chunker)\n",
    "    output = chunker.parse(tokens_tag)\n",
    "    print(\"After Chunking\",output)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(line)\n",
    "    print(\"N_grams\")\n",
    "    print(\"2-gram: \", extract_ngrams(line, 2))\n",
    "    print('\\n')\n",
    "    \n",
    "    line= [word for word in line if word in words]#taking dictionary words only to build final bag of word models\n",
    "    line = [word for word in line if len(word)>1]\n",
    "\n",
    "\n",
    "\n",
    "    line=' '.join(line)\n",
    "    if(line):\n",
    "        corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graphic',\n",
       " 'path da news near net sun sun el',\n",
       " 'peter el peter van veen',\n",
       " 'subject font',\n",
       " 'id peter el',\n",
       " 'line',\n",
       " 'sender news sun',\n",
       " 'organ',\n",
       " 'do',\n",
       " 'refer',\n",
       " 'date',\n",
       " 'say',\n",
       " 'seen sever ray trace scene',\n",
       " 'stroke font appear object',\n",
       " 'font char color depth even',\n",
       " 'wonder',\n",
       " 'thank',\n",
       " 'noel',\n",
       " 'ye server program convert font file',\n",
       " 'font object consist sphere cone',\n",
       " 'use program forgot name place look',\n",
       " 'convert font three differ vivid',\n",
       " 'like faster use express',\n",
       " 'program lot nice',\n",
       " 'interest give mail',\n",
       " 'peter el',\n",
       " 'depart genet',\n",
       " 'auto auto tech alt auto auto',\n",
       " 'path da news near net an net ame sun news sun corp sun',\n",
       " 'sun mark barn',\n",
       " 'auto auto tech alt auto auto',\n",
       " 'subject warn read',\n",
       " 'date',\n",
       " 'organ sun',\n",
       " 'line',\n",
       " 'world',\n",
       " 'id corp sun',\n",
       " 'refer spool mu',\n",
       " 'sun',\n",
       " 'post host corp sun',\n",
       " 'san recent human',\n",
       " 'pour oil road go would',\n",
       " 'would walk right make',\n",
       " 'demand known',\n",
       " 'mark barn system insert standard disclaim',\n",
       " 'technic speak',\n",
       " 'park ca work',\n",
       " 'corp sun',\n",
       " 'environ invest talk environ talk space space rural',\n",
       " 'environ invest talk environ talk space space rural chat teacher',\n",
       " 'path da news near net an net sura net',\n",
       " 'subject space market would wonderful',\n",
       " 'id',\n",
       " 'sender anon anonym post',\n",
       " 'organ southwestern',\n",
       " 'refer fox may graphic se may ca may ca',\n",
       " 'date mon may',\n",
       " 'line',\n",
       " 'stang ca write',\n",
       " 'point well taken still sad idea',\n",
       " 'concern though number reason',\n",
       " 'space number',\n",
       " 'reason probabl doom fail ground',\n",
       " 'read write thread way',\n",
       " 'back space',\n",
       " 'starter think light',\n",
       " 'would bright full moon seem bit',\n",
       " 'propaganda part wish think',\n",
       " 'part',\n",
       " 'second ruin night sky',\n",
       " 'level project either light',\n",
       " 'light',\n",
       " 'target area',\n",
       " 'may may solar power',\n",
       " 'think josh actual math show',\n",
       " 'bright end two month',\n",
       " 'opposit part dark sky activist type',\n",
       " 'project like orbit mirror test',\n",
       " 'recent like point',\n",
       " 'scatter light target area one',\n",
       " 'mirror would wast far project would',\n",
       " 'concern project like would work',\n",
       " 'anyway given like target think',\n",
       " 'go much inhabit much',\n",
       " 'dark sky northern winter doubt find',\n",
       " 'mani activist demand sky back',\n",
       " 'probabl strip buck nake front lamp',\n",
       " 'make sure get enough vitamin day',\n",
       " 'mirror crass',\n",
       " 'think build one one',\n",
       " 'thing affect area think',\n",
       " 'live without doubt go',\n",
       " 'stop',\n",
       " 'number one good faith convert',\n",
       " 'demon anon']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging abbreviations\n",
    "# Abbreviation\tMeaning\n",
    "# CC\tcoordinating conjunction\n",
    "# CD\tcardinal digit\n",
    "# DT\tdeterminer\n",
    "# EX\texistential there\n",
    "# FW\tforeign word\n",
    "# IN\tpreposition/subordinating conjunction\n",
    "# JJ\tadjective (large)\n",
    "# JJR\tadjective, comparative (larger)\n",
    "# JJS\tadjective, superlative (largest)\n",
    "# LS\tlist market\n",
    "# MD\tmodal (could, will)\n",
    "# NN\tnoun, singular (cat, tree)\n",
    "# NNS\tnoun plural (desks)\n",
    "# NNP\tproper noun, singular (sarah)\n",
    "# NNPS\tproper noun, plural (indians or americans)\n",
    "# PDT\tpredeterminer (all, both, half)\n",
    "# POS\tpossessive ending (parent\\ 's)\n",
    "# PRP\tpersonal pronoun (hers, herself, him,himself)\n",
    "# PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "# RB\tadverb (occasionally, swiftly)\n",
    "# RBR\tadverb, comparative (greater)\n",
    "# RBS\tadverb, superlative (biggest)\n",
    "# RP\tparticle (about)\n",
    "# TO\tinfinite marker (to)\n",
    "# UH\tinterjection (goodbye)\n",
    "# VB\tverb (ask)\n",
    "# VBG\tverb gerund (judging)\n",
    "# VBD\tverb past tense (pleaded)\n",
    "# VBN\tverb past participle (reunified)\n",
    "# VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "# VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "# WDT\twh-determiner (that, what)\n",
    "# WP\twh- pronoun (who)\n",
    "# WRB\twh- adverb (how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2034, 34118)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cats=['comp.graphics','talk.religion.misc','sci.space','alt.atheism']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train',categories = cats)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames.shape\n",
    "\n",
    "newsgroups_train.target.shape\n",
    "\n",
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.0132743362832"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8821359240272957"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories = cats)\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "clf = MultinomialNB(alpha = .01)\n",
    "\n",
    "clf.fit(vectors,newsgroups_train.target)\n",
    "\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target,pred,average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: edu it and in you that is of to the\n",
      "comp.graphics: edu in graphics it is for and of to the\n",
      "sci.space: edu it that is in and space to of the\n",
      "talk.religion.misc: not it you in is that and to of the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "        \n",
    "show_top10(clf, vectorizer, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7731035068127478"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'),categories = cats)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(pred, newsgroups_test.target, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19102416446132192"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'),categories=categories)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

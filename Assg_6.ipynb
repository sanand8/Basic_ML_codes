{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os                                                       \n",
    "import re                                                       \n",
    "import random                                                   \n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize                         \n",
    "from nltk.corpus import stopwords                               \n",
    "from nltk.stem.porter import *                                  \n",
    "from nltk.stem.snowball import SnowballStemmer                  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "from nltk.util import ngrams\n",
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['comp.graphics','rec.autos','sci.space']\n",
    "\n",
    "datadir = \"20_newsgroups\"\n",
    "os.path.join(datadir, categories[0])\n",
    "\n",
    "datadir = \"20_newsgroups\"\n",
    "\n",
    "paths=[]\n",
    "l=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Xref: cantaloupe.srv.cs.cmu.edu comp.graphics:38530 comp.sys.amiga.graphics:14070 comp.graphics.animation:3042',\n",
       "  'Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!howland.reston.ans.net!noc.near.net!news.centerline.com!uunet!orca!cnn.sim.es.com!javelin!dingebre',\n",
       "  'From: dingebre@imp.sim.es.com (David Ingebretsen)',\n",
       "  'Newsgroups: comp.graphics,comp.sys.amiga.graphics,comp.graphics.animation',\n",
       "  'Subject: Re: images of earth',\n",
       "  'Date: 20 Apr 1993 18:55:59 GMT',\n",
       "  'Organization: Evans & Sutherland Computer Corp., Salt Lake City, UT',\n",
       "  'Lines: 20',\n",
       "  'Distribution: world',\n",
       "  'Message-ID: <1r1gvv$cit@cnn.sim.es.com>',\n",
       "  'References: <1993Apr19.144533.6779@cs.ruu.nl> <1993Apr19.193758.12091@unocal.com> <1993Apr20.143434.5069@cs.ruu.nl> <1r1d1gINN7vd@gap.caltech.edu>',\n",
       "  'Reply-To: dingebre@imp.sim.es.com (David Ingebretsen)',\n",
       "  'NNTP-Posting-Host: imp.sim.es.com',\n",
       "  '',\n",
       "  'I downloaded an image of the earth re-constructed from elevation data taken',\n",
       "  'at 1/2 degree increments. The author (not me) wrote some c-code (included)',\n",
       "  'that read in the data file and generated b&w and pseudo color images. They',\n",
       "  'work very well and are not incumbered by copyright. They are at an aminet',\n",
       "  'site near you called earth.lha in the amiga/pix/misc area...',\n",
       "  '',\n",
       "  'I refer you to the included docs for the details on how the author (sorry, I',\n",
       "  'forget his name) created these images. The raw data is not included.',\n",
       "  '',\n",
       "  '-- ',\n",
       "  '\\tDavid',\n",
       "  '',\n",
       "  '\\tDavid M. Ingebretsen',\n",
       "  '\\tEvans & Sutherland Computer Corp.',\n",
       "  '\\tdingebre@thunder.sim.es.com',\n",
       "  '',\n",
       "  '\\tDisclaimer: The content of this message in no way reflects the',\n",
       "  '\\t            opinions of my employer, nor are my actions',\n",
       "  '\\t\\t    encouraged, supported, or acknowledged by my',\n",
       "  '\\t\\t    employer.'],\n",
       " ['Newsgroups: rec.autos',\n",
       "  'Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!news.intercon.com!psinntp!telxon.mis.telxon.com!joes',\n",
       "  'From: joes@telxon.mis.telxon.com (Joe Staudt)',\n",
       "  'Subject: Re: V4 V6 V8 V12 Vx?',\n",
       "  'Message-ID: <1993Apr23.172824.17128@telxon.mis.telxon.com>',\n",
       "  'Organization: TELXON Corporation',\n",
       "  'References: <Apr22.202724.24131@engr.washington.edu> <1993Apr23.132214.6755@cs.tulane.edu> <1r8ufk$fr7@usenet.INS.CWRU.Edu>',\n",
       "  'Date: Fri, 23 Apr 1993 17:28:24 GMT',\n",
       "  'Lines: 30',\n",
       "  '',\n",
       "  'In article <1r8ufk$fr7@usenet.INS.CWRU.Edu> aas7@po.CWRU.Edu (Andrew A. ',\n",
       "  'Spencer) writes:',\n",
       "  '>',\n",
       "  '>In a previous article, finnegan@nrlssc.navy.mil () says:',\n",
       "  '>',\n",
       "  '>>In article <Apr22.202724.24131@engr.washington.edu>',\n",
       "  '>>eliot@stalfos.engr.washington.edu (eliot) writes:',\n",
       "  '>>>',\n",
       "  '>>>the subarus all use 180 degree vees in their engines..  :-)',\n",
       "  '>>>',\n",
       "  '>>>',\n",
       "  '>>>eliot',\n",
       "  '>>',\n",
       "  \">>Wouldn't that make them an I4?  Or would they \",\n",
       "  '>>really be an _4 (henceforth referred to as',\n",
       "  '>>\"underscore 4\")?',\n",
       "  '>',\n",
       "  '>i think that it is technicaly known as a 180 degree vee configuration.',\n",
       "  \">(could be wrong....this is how i've seen them referred to)\",\n",
       "  '>DREW',\n",
       "  '',\n",
       "  'I\\'ve always heard them referred to \"horizontally opposed\"...',\n",
       "  '',\n",
       "  'Joe',\n",
       "  '',\n",
       "  '-- ',\n",
       "  'Joseph Staudt, Telxon Corp. | joes@telxon.com',\n",
       "  'P.O. Box 5582               | \"Usenet is like Tetris for people who still',\n",
       "  'Akron, OH  44334-0582       |  remember how to read.\"',\n",
       "  '(216) 867-3700 x3522        |           -- J. Heller'],\n",
       " ['Newsgroups: sci.space',\n",
       "  'Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!bb3.andrew.cmu.edu!crabapple.srv.cs.cmu.edu!chico@ccsun.unicamp.br',\n",
       "  'From: chico@ccsun.unicamp.br (Francisco da Fonseca Rodrigues)',\n",
       "  'Subject: New planet/Kuiper object found?',\n",
       "  'Message-ID: <C5w5zD.3zK.1@cs.cmu.edu>',\n",
       "  'X-Added: Forwarded by Space Digest',\n",
       "  'Sender: news+@cs.cmu.edu',\n",
       "  'Organization: [via International Space University]',\n",
       "  'Original-Sender: isu@VACATION.VENARI.CS.CMU.EDU',\n",
       "  'Distribution: sci',\n",
       "  'Date: Thu, 22 Apr 1993 15:19:42 GMT',\n",
       "  'Approved: bboard-news_gateway',\n",
       "  'Lines: 28',\n",
       "  '',\n",
       "  '',\n",
       "  '\\tTonigth a TV journal here in Brasil announced that an object,',\n",
       "  \"beyond Pluto's orbit, was found by an observatory at Hawaii. They\",\n",
       "  'named the object Karla.',\n",
       "  '',\n",
       "  \"\\tThe program said the object wasn't a gaseous giant planet, and\",\n",
       "  'should be composed by rocks and ices.',\n",
       "  '',\n",
       "  '\\tCan someone confirm these information? Could this object be a',\n",
       "  'new planet or a Kuiper object?',\n",
       "  '',\n",
       "  '\\tThanks in advance.',\n",
       "  '',\n",
       "  '\\tFrancisco.',\n",
       "  '',\n",
       "  '-----------------------=====================================----the stars,----',\n",
       "  '|    ._,               | Francisco da Fonseca Rodrigues    |       o   o     |',\n",
       "  '|  ,_| |._/\\\\           |                                   |     o         o |',\n",
       "  '|  |       |o/^^~-._   | COTUCA-Colegio Tecnico da UNICAMP |   o             |',\n",
       "  \"|/-'    BRASIL      | ~|                                   |  o      o o     |\",\n",
       "  \"|\\\\__/|_            /'  | Depto de Processamento de Dados   |  o    o  o  o   |\",\n",
       "  '|      \\\\__  Cps   | .  |                                   |   o  o  o    o  |',\n",
       "  \"|        |   * __/'    | InterNet : chico@ccsun.unicamp.br |     o o      o  |\",\n",
       "  \"|        >   /'        |            cotuca@ccvax.unicamp.br|             o   |\",\n",
       "  \"|      /'   /'         | Fone/Fax : 55-0192-32-9519        | o         o     |\",\n",
       "  \"|     ~~^\\\\/'           | Campinas - SP - Brasil            |    o   o        |\",\n",
       "  '-----------------------=====================================----like dust.----',\n",
       "  '']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for category in categories:\n",
    "    path = os.path.join(datadir, category)\n",
    "    paths.append(path)\n",
    "\n",
    "for path in paths:\n",
    "    for i in range(1):\n",
    "        choice = random.choice(os.listdir(path)) \n",
    "        fullfilename = os.path.join(path, choice)\n",
    "        with open(fullfilename) as f:\n",
    "            l.append(f.read().splitlines())\n",
    "\n",
    "l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Xref: cantaloupe.srv.cs.cmu.edu comp.graphics:38530 comp.sys.amiga.graphics:14070 comp.graphics.animation:3042',\n",
       " 'Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!howland.reston.ans.net!noc.near.net!news.centerline.com!uunet!orca!cnn.sim.es.com!javelin!dingebre',\n",
       " 'From: dingebre@imp.sim.es.com (David Ingebretsen)',\n",
       " 'Newsgroups: comp.graphics,comp.sys.amiga.graphics,comp.graphics.animation',\n",
       " 'Subject: Re: images of earth',\n",
       " 'Date: 20 Apr 1993 18:55:59 GMT',\n",
       " 'Organization: Evans & Sutherland Computer Corp., Salt Lake City, UT',\n",
       " 'Lines: 20',\n",
       " 'Distribution: world',\n",
       " 'Message-ID: <1r1gvv$cit@cnn.sim.es.com>',\n",
       " 'References: <1993Apr19.144533.6779@cs.ruu.nl> <1993Apr19.193758.12091@unocal.com> <1993Apr20.143434.5069@cs.ruu.nl> <1r1d1gINN7vd@gap.caltech.edu>',\n",
       " 'Reply-To: dingebre@imp.sim.es.com (David Ingebretsen)',\n",
       " 'NNTP-Posting-Host: imp.sim.es.com',\n",
       " '',\n",
       " 'I downloaded an image of the earth re-constructed from elevation data taken',\n",
       " 'at 1/2 degree increments. The author (not me) wrote some c-code (included)',\n",
       " 'that read in the data file and generated b&w and pseudo color images. They',\n",
       " 'work very well and are not incumbered by copyright. They are at an aminet',\n",
       " 'site near you called earth.lha in the amiga/pix/misc area...',\n",
       " '',\n",
       " 'I refer you to the included docs for the details on how the author (sorry, I',\n",
       " 'forget his name) created these images. The raw data is not included.',\n",
       " '',\n",
       " '-- ',\n",
       " '\\tDavid',\n",
       " '',\n",
       " '\\tDavid M. Ingebretsen',\n",
       " '\\tEvans & Sutherland Computer Corp.',\n",
       " '\\tdingebre@thunder.sim.es.com',\n",
       " '',\n",
       " '\\tDisclaimer: The content of this message in no way reflects the',\n",
       " '\\t            opinions of my employer, nor are my actions',\n",
       " '\\t\\t    encouraged, supported, or acknowledged by my',\n",
       " '\\t\\t    employer.',\n",
       " 'Newsgroups: rec.autos',\n",
       " 'Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!news.intercon.com!psinntp!telxon.mis.telxon.com!joes',\n",
       " 'From: joes@telxon.mis.telxon.com (Joe Staudt)',\n",
       " 'Subject: Re: V4 V6 V8 V12 Vx?',\n",
       " 'Message-ID: <1993Apr23.172824.17128@telxon.mis.telxon.com>',\n",
       " 'Organization: TELXON Corporation',\n",
       " 'References: <Apr22.202724.24131@engr.washington.edu> <1993Apr23.132214.6755@cs.tulane.edu> <1r8ufk$fr7@usenet.INS.CWRU.Edu>',\n",
       " 'Date: Fri, 23 Apr 1993 17:28:24 GMT',\n",
       " 'Lines: 30',\n",
       " '',\n",
       " 'In article <1r8ufk$fr7@usenet.INS.CWRU.Edu> aas7@po.CWRU.Edu (Andrew A. ',\n",
       " 'Spencer) writes:',\n",
       " '>',\n",
       " '>In a previous article, finnegan@nrlssc.navy.mil () says:',\n",
       " '>',\n",
       " '>>In article <Apr22.202724.24131@engr.washington.edu>',\n",
       " '>>eliot@stalfos.engr.washington.edu (eliot) writes:',\n",
       " '>>>',\n",
       " '>>>the subarus all use 180 degree vees in their engines..  :-)',\n",
       " '>>>',\n",
       " '>>>',\n",
       " '>>>eliot',\n",
       " '>>',\n",
       " \">>Wouldn't that make them an I4?  Or would they \",\n",
       " '>>really be an _4 (henceforth referred to as',\n",
       " '>>\"underscore 4\")?',\n",
       " '>',\n",
       " '>i think that it is technicaly known as a 180 degree vee configuration.',\n",
       " \">(could be wrong....this is how i've seen them referred to)\",\n",
       " '>DREW',\n",
       " '',\n",
       " 'I\\'ve always heard them referred to \"horizontally opposed\"...',\n",
       " '',\n",
       " 'Joe',\n",
       " '',\n",
       " '-- ',\n",
       " 'Joseph Staudt, Telxon Corp. | joes@telxon.com',\n",
       " 'P.O. Box 5582               | \"Usenet is like Tetris for people who still',\n",
       " 'Akron, OH  44334-0582       |  remember how to read.\"',\n",
       " '(216) 867-3700 x3522        |           -- J. Heller',\n",
       " 'Newsgroups: sci.space',\n",
       " 'Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!bb3.andrew.cmu.edu!crabapple.srv.cs.cmu.edu!chico@ccsun.unicamp.br',\n",
       " 'From: chico@ccsun.unicamp.br (Francisco da Fonseca Rodrigues)',\n",
       " 'Subject: New planet/Kuiper object found?',\n",
       " 'Message-ID: <C5w5zD.3zK.1@cs.cmu.edu>',\n",
       " 'X-Added: Forwarded by Space Digest',\n",
       " 'Sender: news+@cs.cmu.edu',\n",
       " 'Organization: [via International Space University]',\n",
       " 'Original-Sender: isu@VACATION.VENARI.CS.CMU.EDU',\n",
       " 'Distribution: sci',\n",
       " 'Date: Thu, 22 Apr 1993 15:19:42 GMT',\n",
       " 'Approved: bboard-news_gateway',\n",
       " 'Lines: 28',\n",
       " '',\n",
       " '',\n",
       " '\\tTonigth a TV journal here in Brasil announced that an object,',\n",
       " \"beyond Pluto's orbit, was found by an observatory at Hawaii. They\",\n",
       " 'named the object Karla.',\n",
       " '',\n",
       " \"\\tThe program said the object wasn't a gaseous giant planet, and\",\n",
       " 'should be composed by rocks and ices.',\n",
       " '',\n",
       " '\\tCan someone confirm these information? Could this object be a',\n",
       " 'new planet or a Kuiper object?',\n",
       " '',\n",
       " '\\tThanks in advance.',\n",
       " '',\n",
       " '\\tFrancisco.',\n",
       " '',\n",
       " '-----------------------=====================================----the stars,----',\n",
       " '|    ._,               | Francisco da Fonseca Rodrigues    |       o   o     |',\n",
       " '|  ,_| |._/\\\\           |                                   |     o         o |',\n",
       " '|  |       |o/^^~-._   | COTUCA-Colegio Tecnico da UNICAMP |   o             |',\n",
       " \"|/-'    BRASIL      | ~|                                   |  o      o o     |\",\n",
       " \"|\\\\__/|_            /'  | Depto de Processamento de Dados   |  o    o  o  o   |\",\n",
       " '|      \\\\__  Cps   | .  |                                   |   o  o  o    o  |',\n",
       " \"|        |   * __/'    | InterNet : chico@ccsun.unicamp.br |     o o      o  |\",\n",
       " \"|        >   /'        |            cotuca@ccvax.unicamp.br|             o   |\",\n",
       " \"|      /'   /'         | Fone/Fax : 55-0192-32-9519        | o         o     |\",\n",
       " \"|     ~~^\\\\/'           | Campinas - SP - Brasil            |    o   o        |\",\n",
       " '-----------------------=====================================----like dust.----',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting into flat_list\n",
    "lines = [] \n",
    "for sublist in l:\n",
    "    for item in sublist:\n",
    "        lines.append(item)\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\devil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\devil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\devil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "# Generating n-grams from sentences.\n",
    "def extract_ngrams(line, num):\n",
    "    n_grams = ngrams(line, num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized words\n",
      "['xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'comp', 'graphics', 'comp', 'sys', 'amiga', 'graphics', 'comp', 'graphics', 'animation']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'comp', 'graphics', 'comp', 'sys', 'amiga', 'graphics', 'comp', 'graphics', 'animation']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['xref', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'comp', 'graphic', 'comp', 'sys', 'amiga', 'graphic', 'comp', 'graphic', 'animation']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'comp', 'graphic', 'comp', 'sy', 'amiga', 'graphic', 'comp', 'graphic', 'anim']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'comp', 'graphic', 'comp', 'sy', 'amiga', 'graphic', 'comp', 'graphic', 'anim']\n",
      "After pos_tags: [('xref', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('comp', 'NN'), ('graphic', 'JJ'), ('comp', 'NN'), ('sy', 'NN'), ('amiga', 'NN'), ('graphic', 'JJ'), ('comp', 'NN'), ('graphic', 'JJ'), ('anim', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk xref/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN comp/NN graphic/JJ)\n",
      "  (mychunk comp/NN sy/NN amiga/NN graphic/JJ)\n",
      "  (mychunk comp/NN graphic/JJ)\n",
      "  (mychunk anim/NN))\n",
      "\n",
      "\n",
      "['xref', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'comp', 'graphic', 'comp', 'sy', 'amiga', 'graphic', 'comp', 'graphic', 'anim']\n",
      "N_grams\n",
      "2-gram:  ['xref cantaloup', 'cantaloup srv', 'srv c', 'c cmu', 'cmu edu', 'edu comp', 'comp graphic', 'graphic comp', 'comp sy', 'sy amiga', 'amiga graphic', 'graphic comp', 'comp graphic', 'graphic anim']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'crabapple', 'srv', 'cs', 'cmu', 'edu', 'fs', 'ece', 'cmu', 'edu', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'ans', 'net', 'noc', 'near', 'net', 'news', 'centerline', 'com', 'uunet', 'orca', 'cnn', 'sim', 'es', 'com', 'javelin', 'dingebre']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'crabapple', 'srv', 'cs', 'cmu', 'edu', 'fs', 'ece', 'cmu', 'edu', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'ans', 'net', 'noc', 'near', 'net', 'news', 'centerline', 'com', 'uunet', 'orca', 'cnn', 'sim', 'es', 'com', 'javelin', 'dingebre']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['path', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'crabapple', 'srv', 'c', 'cmu', 'edu', 'f', 'ece', 'cmu', 'edu', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'an', 'net', 'noc', 'near', 'net', 'news', 'centerline', 'com', 'uunet', 'orca', 'cnn', 'sim', 'e', 'com', 'javelin', 'dingebre']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'crabappl', 'srv', 'c', 'cmu', 'edu', 'f', 'ece', 'cmu', 'edu', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'an', 'net', 'noc', 'near', 'net', 'news', 'centerlin', 'com', 'uunet', 'orca', 'cnn', 'sim', 'e', 'com', 'javelin', 'dingebr']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'crabappl', 'srv', 'c', 'cmu', 'edu', 'f', 'ece', 'cmu', 'edu', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'an', 'net', 'noc', 'near', 'net', 'news', 'centerlin', 'com', 'uunet', 'orca', 'cnn', 'sim', 'e', 'com', 'javelin', 'dingebr']\n",
      "After pos_tags: [('path', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('crabappl', 'NN'), ('srv', 'NN'), ('c', 'NN'), ('cmu', 'NN'), ('edu', 'NN'), ('f', 'JJ'), ('ece', 'NN'), ('cmu', 'NN'), ('edu', 'NN'), ('europa', 'NN'), ('eng', 'NN'), ('gtefsd', 'NN'), ('com', 'NN'), ('howland', 'NN'), ('reston', 'VBZ'), ('an', 'DT'), ('net', 'JJ'), ('noc', 'NN'), ('near', 'IN'), ('net', 'JJ'), ('news', 'NN'), ('centerlin', 'NN'), ('com', 'NN'), ('uunet', 'JJ'), ('orca', 'NN'), ('cnn', 'NN'), ('sim', 'NN'), ('e', 'NN'), ('com', 'NN'), ('javelin', 'NN'), ('dingebr', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk path/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN crabappl/NN srv/NN c/NN cmu/NN edu/NN f/JJ)\n",
      "  (mychunk\n",
      "    ece/NN\n",
      "    cmu/NN\n",
      "    edu/NN\n",
      "    europa/NN\n",
      "    eng/NN\n",
      "    gtefsd/NN\n",
      "    com/NN\n",
      "    howland/NN)\n",
      "  reston/VBZ\n",
      "  an/DT\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk noc/NN)\n",
      "  near/IN\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk news/NN centerlin/NN com/NN uunet/JJ)\n",
      "  (mychunk orca/NN cnn/NN sim/NN e/NN com/NN javelin/NN dingebr/NN))\n",
      "\n",
      "\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'crabappl', 'srv', 'c', 'cmu', 'edu', 'f', 'ece', 'cmu', 'edu', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'an', 'net', 'noc', 'near', 'net', 'news', 'centerlin', 'com', 'uunet', 'orca', 'cnn', 'sim', 'e', 'com', 'javelin', 'dingebr']\n",
      "N_grams\n",
      "2-gram:  ['path cantaloup', 'cantaloup srv', 'srv c', 'c cmu', 'cmu edu', 'edu crabappl', 'crabappl srv', 'srv c', 'c cmu', 'cmu edu', 'edu f', 'f ece', 'ece cmu', 'cmu edu', 'edu europa', 'europa eng', 'eng gtefsd', 'gtefsd com', 'com howland', 'howland reston', 'reston an', 'an net', 'net noc', 'noc near', 'near net', 'net news', 'news centerlin', 'centerlin com', 'com uunet', 'uunet orca', 'orca cnn', 'cnn sim', 'sim e', 'e com', 'com javelin', 'javelin dingebr']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['from', 'dingebre', 'imp', 'sim', 'es', 'com', 'david', 'ingebretsen']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['dingebre', 'imp', 'sim', 'es', 'com', 'david', 'ingebretsen']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['dingebre', 'imp', 'sim', 'e', 'com', 'david', 'ingebretsen']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['dingebr', 'imp', 'sim', 'e', 'com', 'david', 'ingebretsen']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['dingebr', 'imp', 'sim', 'e', 'com', 'david', 'ingebretsen']\n",
      "After pos_tags: [('dingebr', 'NN'), ('imp', 'NN'), ('sim', 'NN'), ('e', 'NN'), ('com', 'NN'), ('david', 'NN'), ('ingebretsen', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk\n",
      "    dingebr/NN\n",
      "    imp/NN\n",
      "    sim/NN\n",
      "    e/NN\n",
      "    com/NN\n",
      "    david/NN\n",
      "    ingebretsen/NN))\n",
      "\n",
      "\n",
      "['dingebr', 'imp', 'sim', 'e', 'com', 'david', 'ingebretsen']\n",
      "N_grams\n",
      "2-gram:  ['dingebr imp', 'imp sim', 'sim e', 'e com', 'com david', 'david ingebretsen']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['newsgroups', 'comp', 'graphics', 'comp', 'sys', 'amiga', 'graphics', 'comp', 'graphics', 'animation']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['newsgroups', 'comp', 'graphics', 'comp', 'sys', 'amiga', 'graphics', 'comp', 'graphics', 'animation']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['newsgroups', 'comp', 'graphic', 'comp', 'sys', 'amiga', 'graphic', 'comp', 'graphic', 'animation']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['newsgroup', 'comp', 'graphic', 'comp', 'sy', 'amiga', 'graphic', 'comp', 'graphic', 'anim']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['newsgroup', 'comp', 'graphic', 'comp', 'sy', 'amiga', 'graphic', 'comp', 'graphic', 'anim']\n",
      "After pos_tags: [('newsgroup', 'JJ'), ('comp', 'NN'), ('graphic', 'JJ'), ('comp', 'NN'), ('sy', 'NN'), ('amiga', 'NN'), ('graphic', 'JJ'), ('comp', 'NN'), ('graphic', 'JJ'), ('anim', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk newsgroup/JJ)\n",
      "  (mychunk comp/NN graphic/JJ)\n",
      "  (mychunk comp/NN sy/NN amiga/NN graphic/JJ)\n",
      "  (mychunk comp/NN graphic/JJ)\n",
      "  (mychunk anim/NN))\n",
      "\n",
      "\n",
      "['newsgroup', 'comp', 'graphic', 'comp', 'sy', 'amiga', 'graphic', 'comp', 'graphic', 'anim']\n",
      "N_grams\n",
      "2-gram:  ['newsgroup comp', 'comp graphic', 'graphic comp', 'comp sy', 'sy amiga', 'amiga graphic', 'graphic comp', 'comp graphic', 'graphic anim']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['subject', 're', 'images', 'of', 'earth']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['subject', 'images', 'earth']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['subject', 'image', 'earth']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['subject', 'imag', 'earth']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['subject', 'imag', 'earth']\n",
      "After pos_tags: [('subject', 'JJ'), ('imag', 'NN'), ('earth', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk subject/JJ) (mychunk imag/NN earth/NN))\n",
      "\n",
      "\n",
      "['subject', 'imag', 'earth']\n",
      "N_grams\n",
      "2-gram:  ['subject imag', 'imag earth']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['date', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['date', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['date', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['date', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['date', 'apr', 'gmt']\n",
      "After pos_tags: [('date', 'NN'), ('apr', 'NNS'), ('gmt', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk date/NN apr/NNS) gmt/VBP)\n",
      "\n",
      "\n",
      "['date', 'apr', 'gmt']\n",
      "N_grams\n",
      "2-gram:  ['date apr', 'apr gmt']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['organization', 'evans', 'sutherland', 'computer', 'corp', 'salt', 'lake', 'city', 'ut']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['organization', 'evans', 'sutherland', 'computer', 'corp', 'salt', 'lake', 'city', 'ut']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['organization', 'evans', 'sutherland', 'computer', 'corp', 'salt', 'lake', 'city', 'ut']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['organ', 'evan', 'sutherland', 'comput', 'corp', 'salt', 'lake', 'citi', 'ut']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['organ', 'evan', 'sutherland', 'comput', 'corp', 'salt', 'lake', 'citi', 'ut']\n",
      "After pos_tags: [('organ', 'JJ'), ('evan', 'JJ'), ('sutherland', 'NN'), ('comput', 'VBD'), ('corp', 'JJ'), ('salt', 'NN'), ('lake', 'NN'), ('citi', 'NN'), ('ut', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk organ/JJ evan/JJ)\n",
      "  (mychunk sutherland/NN comput/VBD corp/JJ)\n",
      "  (mychunk salt/NN lake/NN citi/NN ut/NN))\n",
      "\n",
      "\n",
      "['organ', 'evan', 'sutherland', 'comput', 'corp', 'salt', 'lake', 'citi', 'ut']\n",
      "N_grams\n",
      "2-gram:  ['organ evan', 'evan sutherland', 'sutherland comput', 'comput corp', 'corp salt', 'salt lake', 'lake citi', 'citi ut']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['lines']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['lines']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['line']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['line']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['line']\n",
      "After pos_tags: [('line', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk line/NN))\n",
      "\n",
      "\n",
      "['line']\n",
      "N_grams\n",
      "2-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['distribut', 'world']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['distribut', 'world']\n",
      "After pos_tags: [('distribut', 'NN'), ('world', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk distribut/NN world/NN))\n",
      "\n",
      "\n",
      "['distribut', 'world']\n",
      "N_grams\n",
      "2-gram:  ['distribut world']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['message', 'id', 'r', 'gvv', 'cit', 'cnn', 'sim', 'es', 'com']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['message', 'id', 'r', 'gvv', 'cit', 'cnn', 'sim', 'es', 'com']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['message', 'id', 'r', 'gvv', 'cit', 'cnn', 'sim', 'e', 'com']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['messag', 'id', 'r', 'gvv', 'cit', 'cnn', 'sim', 'e', 'com']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['messag', 'id', 'r', 'gvv', 'cit', 'cnn', 'sim', 'e', 'com']\n",
      "After pos_tags: [('messag', 'NN'), ('id', 'NN'), ('r', 'NN'), ('gvv', 'NN'), ('cit', 'NN'), ('cnn', 'NN'), ('sim', 'NN'), ('e', 'NN'), ('com', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk\n",
      "    messag/NN\n",
      "    id/NN\n",
      "    r/NN\n",
      "    gvv/NN\n",
      "    cit/NN\n",
      "    cnn/NN\n",
      "    sim/NN\n",
      "    e/NN\n",
      "    com/NN))\n",
      "\n",
      "\n",
      "['messag', 'id', 'r', 'gvv', 'cit', 'cnn', 'sim', 'e', 'com']\n",
      "N_grams\n",
      "2-gram:  ['messag id', 'id r', 'r gvv', 'gvv cit', 'cit cnn', 'cnn sim', 'sim e', 'e com']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['references', 'apr', 'cs', 'ruu', 'nl', 'apr', 'unocal', 'com', 'apr', 'cs', 'ruu', 'nl', 'r', 'd', 'ginn', 'vd', 'gap', 'caltech', 'edu']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Stopwords\n",
      "['references', 'apr', 'cs', 'ruu', 'nl', 'apr', 'unocal', 'com', 'apr', 'cs', 'ruu', 'nl', 'r', 'ginn', 'vd', 'gap', 'caltech', 'edu']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reference', 'apr', 'c', 'ruu', 'nl', 'apr', 'unocal', 'com', 'apr', 'c', 'ruu', 'nl', 'r', 'ginn', 'vd', 'gap', 'caltech', 'edu']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['refer', 'apr', 'c', 'ruu', 'nl', 'apr', 'unoc', 'com', 'apr', 'c', 'ruu', 'nl', 'r', 'ginn', 'vd', 'gap', 'caltech', 'edu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['refer', 'apr', 'c', 'ruu', 'nl', 'apr', 'unoc', 'com', 'apr', 'c', 'ruu', 'nl', 'r', 'ginn', 'vd', 'gap', 'caltech', 'edu']\n",
      "After pos_tags: [('refer', 'NN'), ('apr', 'VBZ'), ('c', 'JJ'), ('ruu', 'NN'), ('nl', 'JJ'), ('apr', 'NN'), ('unoc', 'JJ'), ('com', 'NN'), ('apr', 'NN'), ('c', 'NN'), ('ruu', 'NN'), ('nl', 'JJ'), ('r', 'NN'), ('ginn', 'NN'), ('vd', 'NN'), ('gap', 'NN'), ('caltech', 'NN'), ('edu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk refer/NN)\n",
      "  apr/VBZ\n",
      "  (mychunk c/JJ)\n",
      "  (mychunk ruu/NN nl/JJ)\n",
      "  (mychunk apr/NN unoc/JJ)\n",
      "  (mychunk com/NN apr/NN c/NN ruu/NN nl/JJ)\n",
      "  (mychunk r/NN ginn/NN vd/NN gap/NN caltech/NN edu/NN))\n",
      "\n",
      "\n",
      "['refer', 'apr', 'c', 'ruu', 'nl', 'apr', 'unoc', 'com', 'apr', 'c', 'ruu', 'nl', 'r', 'ginn', 'vd', 'gap', 'caltech', 'edu']\n",
      "N_grams\n",
      "2-gram:  ['refer apr', 'apr c', 'c ruu', 'ruu nl', 'nl apr', 'apr unoc', 'unoc com', 'com apr', 'apr c', 'c ruu', 'ruu nl', 'nl r', 'r ginn', 'ginn vd', 'vd gap', 'gap caltech', 'caltech edu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['reply', 'to', 'dingebre', 'imp', 'sim', 'es', 'com', 'david', 'ingebretsen']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['reply', 'dingebre', 'imp', 'sim', 'es', 'com', 'david', 'ingebretsen']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reply', 'dingebre', 'imp', 'sim', 'e', 'com', 'david', 'ingebretsen']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['repli', 'dingebr', 'imp', 'sim', 'e', 'com', 'david', 'ingebretsen']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['repli', 'dingebr', 'imp', 'sim', 'e', 'com', 'david', 'ingebretsen']\n",
      "After pos_tags: [('repli', 'NN'), ('dingebr', 'NN'), ('imp', 'NN'), ('sim', 'NN'), ('e', 'NN'), ('com', 'NN'), ('david', 'NN'), ('ingebretsen', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk\n",
      "    repli/NN\n",
      "    dingebr/NN\n",
      "    imp/NN\n",
      "    sim/NN\n",
      "    e/NN\n",
      "    com/NN\n",
      "    david/NN\n",
      "    ingebretsen/NN))\n",
      "\n",
      "\n",
      "['repli', 'dingebr', 'imp', 'sim', 'e', 'com', 'david', 'ingebretsen']\n",
      "N_grams\n",
      "2-gram:  ['repli dingebr', 'dingebr imp', 'imp sim', 'sim e', 'e com', 'com david', 'david ingebretsen']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['nntp', 'posting', 'host', 'imp', 'sim', 'es', 'com']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['nntp', 'posting', 'host', 'imp', 'sim', 'es', 'com']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['nntp', 'posting', 'host', 'imp', 'sim', 'e', 'com']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['nntp', 'post', 'host', 'imp', 'sim', 'e', 'com']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['nntp', 'post', 'host', 'imp', 'sim', 'e', 'com']\n",
      "After pos_tags: [('nntp', 'RB'), ('post', 'NN'), ('host', 'NN'), ('imp', 'NN'), ('sim', 'NN'), ('e', 'NN'), ('com', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S nntp/RB (mychunk post/NN host/NN imp/NN sim/NN e/NN com/NN))\n",
      "\n",
      "\n",
      "['nntp', 'post', 'host', 'imp', 'sim', 'e', 'com']\n",
      "N_grams\n",
      "2-gram:  ['nntp post', 'post host', 'host imp', 'imp sim', 'sim e', 'e com']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\nltk\\util.py\u001b[0m in \u001b[0;36mngrams\u001b[1;34m(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f34749f70035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"N_grams\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2-gram: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-e0be0a95dc01>\u001b[0m in \u001b[0;36mextract_ngrams\u001b[1;34m(line, num)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_grams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrams\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgrams\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-e0be0a95dc01>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_grams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrams\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgrams\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_grams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from nltk import word_tokenize, pos_tag\n",
    "for i in range(len(lines)):\n",
    "    line=[]\n",
    "    line=re.sub('[^a-zA-Z]',' ',lines[i])\n",
    "    line = line.lower()\n",
    "    line = line.split()#tokenization \n",
    "    print(\"tokenized words\")\n",
    "    print(line)\n",
    "    print(\"\\n\")\n",
    "    line=[word for word in line if word not in set(stopwords.words('english'))]\n",
    "    print(\"Removing Stopwords\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    line = [lemmatizer.lemmatize(word) for word in line]\n",
    "    print(\"Lemmatization\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    line = [ps.stem(word) for word in line]\n",
    "    print(\"Stemming\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    print(\"pos_tags and chunking\")\n",
    "    print(\"After Split:\",line)\n",
    "    tokens_tag = pos_tag(line)\n",
    "    print(\"After pos_tags:\",tokens_tag)\n",
    "    patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "    chunker = RegexpParser(patterns)\n",
    "    print(\"After Regex:\",chunker)\n",
    "    output = chunker.parse(tokens_tag)\n",
    "    print(\"After Chunking\",output)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(line)\n",
    "    print(\"N_grams\")\n",
    "    print(\"2-gram: \", extract_ngrams(line, 2))\n",
    "    print('\\n')\n",
    "    \n",
    "    line= [word for word in line if word in words]#taking dictionary words only to build final bag of word models\n",
    "    line = [word for word in line if len(word)>1]\n",
    "\n",
    "\n",
    "\n",
    "    line=' '.join(line)\n",
    "    if(line):\n",
    "        corpus.append(line)\n",
    "\n",
    "\n",
    "corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cats=['comp.graphics','talk.religion.misc','sci.space','alt.atheism']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train',categories = cats)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape\n",
    "\n",
    "newsgroups_train.filenames.shape\n",
    "\n",
    "newsgroups_train.target.shape\n",
    "\n",
    "newsgroups_train.target[:10]\n",
    "\n",
    "vectors.nnz / float(vectors.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories = cats)\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "clf = MultinomialNB(alpha = .01)\n",
    "\n",
    "clf.fit(vectors,newsgroups_train.target)\n",
    "\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target,pred,average = 'macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "        \n",
    "show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'),categories = cats)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'),categories=categories)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
